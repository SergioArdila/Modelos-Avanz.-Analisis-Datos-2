{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
    "\n",
    "\n",
    "# Introducción a las redes neuronales\n",
    "\n",
    "## Actividad 2\n",
    "\n",
    "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
    "\n",
    "### Estudiantes:\n",
    "\n",
    "- Ivan Galindo Gaviria Cod: 201924193\n",
    "- Jonny Coronel Villamil Cod: 201411692\n",
    "- Sergio Ardila Rodríguez Cod: 201924139\n",
    "\n",
    "En esta actividad vamos a estudiar una primera aproximación a los modelos de redes neuronales, utilizando como base el modelo de regresión logística.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunos paquetes iniciales que vamos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problema de clasificación: riesgo de default\n",
    "\n",
    "Examinemos los datos con lo cuales ya estamos familiarizados:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_1 = pd.read_csv(\"germancredit.csv\")\n",
    "credit_1 = pd.get_dummies(credit_1, columns=['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'], prefix = ['checkingstatus1','history','purpose','savings',\n",
    "                                   'employ','status','others','property','otherplans','housing','job','tele', \n",
    "                                   'foreign'])\n",
    "X = credit_1.iloc[:, 1:62]\n",
    "Y = credit_1.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de CE, CP:  (600,) (400,)\n",
      "Observaciones de la clase positiva en entrenamiento: 180 y en prueba: 120\n"
     ]
    }
   ],
   "source": [
    "CE_x, CP_x, CE_y, CP_y = model_selection.train_test_split(X, Y, test_size=0.4, random_state=42, stratify=Y)\n",
    "print(\"Tamaño de CE, CP: \", CE_y.shape, CP_y.shape)\n",
    "print(\"Observaciones de la clase positiva en entrenamiento: \" +str(sum(CE_y)) +\" y en prueba: \" +str(sum(CP_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de una neurona Sigmoide\n",
    "\n",
    "Una neurona Sigmoide puede ser vista como un perceptrón *suavizado* que recibe una señal y entonces se activa. Al activarse, transforma la señal para entender mejor el mensaje. Esta transformación la ejecuta a partir de la fucnión Sigmoide.\n",
    "\n",
    "Si tomamos la señal como un conjunto de datos de entrada y el mensaje como la predicción de un valor, la función de activación jugará el papel de transformadora de los datos de entrada en aquello que se quiere entender/predecir, que además replica un modelo logit con la función de activación sigmoide.\n",
    "\n",
    "A continuación construiremos un clasificador de regresión logística bajo la perspectiva de una red neuronal, estudiando la arquitectura general de un algoritmo de aprendizaje. De esta manera, necesitaremos incluir la inicialización de los parámetros, el cálculo de la función de coste y su gradiente, y utilizar un algoritmo de optimización como por ejemplo el descenso en la dirección del gradiente (GD)\n",
    "\n",
    "**Formulación del algoritmo**:\n",
    "\n",
    "Para un ejemplo $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoide(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "El coste se calcula sumando sobre todos los ejemplos de entrenamiento:\n",
    "$$ L = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construimos las partes del algoritmo  \n",
    "\n",
    "- Inicializar los parámetros del modelo\n",
    "- Bucle:\n",
    "    - Calcular la pérdida actual (propagación hacia delante)\n",
    "    - Calcular el gradiente actual (retro-propagación)\n",
    "    - Actualizar los parámetros (descenso en la dirección del gradiente)\n",
    "\n",
    "\n",
    "### Ejercicio 2.1\n",
    "Implemente la funcion `sigmoide()` $$\\sigma( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$$ Para ello puede utilizar np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    z: Un escalar o arreglo numpy de cualquier tamaño\n",
    "    Output:\n",
    "    s: sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoide([99,1,0,-1,-99]) = [1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
      " 1.01122149e-43]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoide([99,1,0,-1,-99]) = \" + str(sigmoide(np.array([99,1,0,-1,-99]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td> sigmoide([99,1,0,-1,-99])    = </td>\n",
    "<td> [ 1.00000000e+00 7.31058579e-01 5.00000000e-01 2.68941421e-01\n",
    " 1.01122149e-43] </td> \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.2 \n",
    "\n",
    "Debemos inicializar los parámetros a cero. Puede utilizar la funcion np.zeros(), apoyandose en la documentación de la biblioteca Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_ceros(dim):\n",
    "    \"\"\"\n",
    "    Esta función crea un vector de ceros de dimensión (dim, 1) para w e inicializa b a 0.\n",
    "    Input:\n",
    "    dim: tamaño del vector w (número de parámetros para este caso)\n",
    "    Output:\n",
    "    w: vector inicializado de tamaño (dim, 1)\n",
    "    b: escalar inicializado (corresponde con el sesgo)\n",
    "    \"\"\"\n",
    "    \n",
    "    w = np.zeros([dim,1])\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 6\n",
    "w, b = inicializa_ceros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "<tr>\n",
    "<td>   w   </td>\n",
    "<td> [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   b   </td>\n",
    "<td> 0 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3 \n",
    "#### Propagación hacia delante y hacia atrás\n",
    "\n",
    "Una vez los estimadores están inicializados, se pueden implementar los pasos de propagación hacia \"delante\" y hacia \"atrás\" para el aprendizaje automático. \n",
    "\n",
    "La propagación hacia delante consiste en calcular la función de activación sigmoide sobre la combinacón lineal de los patrones y los coeficientes inciales. \n",
    "\n",
    "Luego la propagación hacia atrás, o *retro-propagación*, es el paso más importante, donde utilizamos el gradiente de la función del error o de pérdida para actualizar los coeficientes. \n",
    "\n",
    "Este procedimiento se repite iterativamente replicando el procediemiento de descenso en la dirección del gradiente o *Gradient Descent* (GD).\n",
    "\n",
    "A continuación implemente la función `propaga()` que calcula la función de coste y su gradiente.\n",
    "\n",
    "**Ayuda**:\n",
    "\n",
    "Propagación hacia delante:\n",
    "- Se tiene $X$\n",
    "- Se calcula $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Se calcula la función de coste/pérdida: $L = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Para la retro-propagación, tenemos que calcular la derivada parcial de *L* con respecto a nuestros coeficientes $(w,b)$:  \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{5}$$\n",
    "$$ \\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{6}$$\n",
    "\n",
    "*Nota:* Para el cálculo de estas derivadas debemos hacer uso de la regla de la cadena. \n",
    "\n",
    "Esto es, dado $Z=w^T X + b$, se tiene que $$\\frac{\\partial L}{\\partial Z} = \\frac{\\partial L}{\\partial A} \\frac{\\partial A}{\\partial Z} = \\bigg(\\frac{-Y}{A}+\\frac{1-Y}{1-A}\\bigg) (A \\cdot (1-A)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propaga(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implemente la función de coste y su gradiente para la propagación\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    Output:\n",
    "    coste: coste negativo de log-verosimilitud para la regresión logística\n",
    "    dw: gradiente de la pérdida con respecto a w, con las mismas dimensiones que w\n",
    "    db: gradiente de la pérdida con respecto a b, con las mismas dimensiones que b\n",
    "    \n",
    "    (Sugerencia: utilice las funciones np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A = sigmoide(np.dot(np.transpose(w),X) + b) # compute la activación\n",
    "    coste = (-1/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A)) # compute el coste\n",
    "\n",
    "    dw = (1/m)*np.dot(X,np.transpose(A-Y))\n",
    "    db = (1/m)*np.sum(A-Y)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(coste)\n",
    "    assert(coste.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[65.48251839]\n",
      " [29.66675568]]\n",
      "db = 0.348980796447886\n",
      "coste = 9.752716367426284\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[0.1],[0.1]]), 0.5, np.array([[66.,99.,-33.],[32.,55.,-2.1]]), np.array([[0,0,1]])\n",
    "grads, coste = propaga(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"coste = \" + str(coste))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\">\n",
    "<tr>\n",
    "<td>   dw   </td>\n",
    "<td> [[65.48251839]\n",
    " [29.66675568]]</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   db   </td>\n",
    "<td> 0.348980796447886 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>   cost   </td>\n",
    "<td> 9.752716367426284 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4 \n",
    "#### Optimización\n",
    "\n",
    "- Se tienen los parámetros inicializados.\n",
    "- También se tiene el código para calcular la función de coste y su gradiente.\n",
    "- Ahora se quieren actualizar los parámetros utilizando el GD.\n",
    "\n",
    "Escriba la función de optimización para aprender $w$ y $b$ minimizando la función de coste $L$. \n",
    "\n",
    "Para un parámetro $\\theta$, la regla de actualización es $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, donde $\\alpha$ es la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimiza(w, b, X, Y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Esta función optimiza w y b implementando el algoritmo de GD\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Y: vector de etiquetas \n",
    "    num_iter: número de iteracionespara el bucle de optimización\n",
    "    tasa: tasa de aprendizaje para la regla de actualización del GD\n",
    "    print_cost: True para imprimir la pérdida cada 100 iteraciones\n",
    "    Output:\n",
    "    params: diccionario con los pesos w y el sesgo b\n",
    "    grads: diccionario con los gradientes de los pesos y el sesgo con respecto a la función de pérdida\n",
    "    costes: lista de todos los costes calculados durante la optimización, usados para graficar la curva de aprendizaje.\n",
    "    \n",
    "    Sugerencia: puede escribir dos pasos e iterar sobre ellos:\n",
    "        1) Calcule el coste y el gradiente de los parámetros actuales. Use propaga().\n",
    "        2) Actualize los parámetros usando la regla del GD para w y b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costes = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        \n",
    "        # Computación del coste y el gradiente \n",
    "        grads, coste = propaga(w, b, X, Y)\n",
    "        \n",
    "        # Recupere las derivadas de grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Actualize la regla \n",
    "        w = w-tasa*dw\n",
    "        b = b-tasa*db\n",
    "        \n",
    "        # Guarde los costes\n",
    "        if i % 100 == 0:\n",
    "            costes.append(coste)\n",
    "        \n",
    "        # Se muestra el coste cada 100 iteraciones de entrenamiento\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Coste tras la iteración %i: %f\" %(i, coste))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-0.07262234]\n",
      " [ 0.02112647]]\n",
      "b = 0.49898148713402446\n",
      "dw = [[1.42076721]\n",
      " [0.43496446]]\n",
      "db = -0.007821662502973652\n"
     ]
    }
   ],
   "source": [
    "params, grads, costes = optimiza(w, b, X, Y, num_iter= 10, tasa = 0.001, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**:  \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> w </td>\n",
    "<td>[[-0.07262234]\n",
    " [ 0.02112647]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> b </td>\n",
    "<td> 0.49898148713402446 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> dw </td>\n",
    "<td> [[1.42076721]\n",
    " [0.43496446]] </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> db </td>\n",
    "<td> -0.007821662502973652 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.5\n",
    "\n",
    "La función anterior aprende los parámetros w y b, que se pueden usar para predecir sobre el conjunto de datos X. \n",
    "\n",
    "Hay dos pasos para calcular las predicciones:\n",
    "\n",
    "1. Calcular $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Converir a 0 las entradas de $a$ (si la activación es <= 0.5) o 1 (si la activación es > 0.5), guarde las predicciones en un vector `Y_pred`.  \n",
    "\n",
    "Ahora implemente la función `pred()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w, b, X):\n",
    "    '''\n",
    "    Prediga si una etiqueta es 0 o 1 usando los parámetros de regresión logística aprendidos (w, b)\n",
    "    Input:\n",
    "    w: pesos, un arreglo numpy \n",
    "    b: sesgo, un escalar\n",
    "    X: datos de entrada\n",
    "    Output:\n",
    "    Y_pred: vector con todas las predicciones (0/1) para los ejemplos en X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_pred = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute el vector \"A\" prediciendo las probabilidades\n",
    "    A = sigmoide(np.dot(np.transpose(w),X)+b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convierta las probabilidades A[0,i] a predicciones p[0,i]\n",
    "        Y_pred[0,i] = round(A[0,i],0)\n",
    "    \n",
    "    assert(Y_pred.shape == (1, m))\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicciones = [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.12],[0.23]])\n",
    "b = -0.09\n",
    "X = np.array([[3.1,-2.9,0.2],[1.9,1.8,-0.09]])\n",
    "print (\"predicciones = \" + str(pred(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:40%\">\n",
    "<tr>\n",
    "<td> predicciones   </td>\n",
    "<td>[[ 1.  0.  0.]]  </td>  \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.6\n",
    "#### Combine todas las funciones \n",
    "\n",
    "Ahora juntemos todos los bloques que ha programado arriba.\n",
    "\n",
    "Implemente la función del modelo \"madre\". Use la siguiente notación:\n",
    "    - YP_pred para las predicciones sobre el conjunto de prueba\n",
    "    - YE_pred para las predicciones sobre el conjunto de entrenamiento\n",
    "    - w, costes, grads para las salidas de optimiza()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo(CE_x, CP_x, CE_y, CP_y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Construye el modelo de regresión logística llamando las funciones implementadas anteriormente\n",
    "    Output:\n",
    "    d: diccionario con la información sobre el modelo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicialice los parametros con ceros \n",
    "    w = np.zeros([CE_x.shape[0],1])\n",
    "    b = 0\n",
    "\n",
    "    # Descenso en la dirección del gradiente (GD) \n",
    "    params, grads, costes = optimiza(w, b, CE_x, CE_y, num_iter, tasa, print_cost)\n",
    "    \n",
    "    # Recupere los parámetros w y b del diccionario \"params\" ##\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Prediga los ejemplos de prueba y entrenamiento (≈ 2 líneas de código)\n",
    "    YP_pred = pred(w, b, CP_x)\n",
    "    YE_pred = pred(w, b, CE_x)\n",
    "\n",
    "    # Imprima los errores de entrenamiento y prueba\n",
    "    print(\"Accuracy de entrenamiento: {} %\".format(100 - np.mean(np.abs(YE_pred - CE_y)) * 100))\n",
    "    print(\"Accuracy de prueba: {} %\".format(100 - np.mean(np.abs(YP_pred - CP_y)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"Costes\": costes,\n",
    "         \"Prediccion_prueba\": YP_pred, \n",
    "         \"Prediccion_entrenamiento\" : YE_pred, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"Tasa de aprendizaje\" : tasa,\n",
    "         \"Numero de iteraciones\": num_iter}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2.7\n",
    "\n",
    "De qué dimensiones deben ser las matrices con los datos de entrada y de salida?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la matriz\n",
      "Con los datos de entrada --> (61, 600)\n",
      "Con los datos de salida --> (1, 600)\n"
     ]
    }
   ],
   "source": [
    "print('Dimensiones de la matriz')\n",
    "print('Con los datos de entrada -->',(np.transpose(CE_x)).shape)\n",
    "print('Con los datos de salida -->',(np.array(CE_y)[np.newaxis]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 600) (1, 600)\n"
     ]
    }
   ],
   "source": [
    "# Podemos re-configurar las matrices de la siguiente forma:\n",
    "CE_x2 = CE_x.T\n",
    "CP_x2 = CP_x.T\n",
    "CE_y2 = np.array(CE_y)[np.newaxis]\n",
    "CP_y2 = np.array(CP_y)[np.newaxis]\n",
    "\n",
    "print(CE_x2.shape, CE_y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ejecute la siguiente celda para entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: 0.693147\n",
      "Coste tras la iteración 100: 0.656075\n",
      "Coste tras la iteración 200: 0.656015\n",
      "Coste tras la iteración 300: 0.656002\n",
      "Coste tras la iteración 400: 0.655989\n",
      "Coste tras la iteración 500: 0.655976\n",
      "Coste tras la iteración 600: 0.655963\n",
      "Coste tras la iteración 700: 0.655950\n",
      "Coste tras la iteración 800: 0.655937\n",
      "Coste tras la iteración 900: 0.655924\n",
      "Coste tras la iteración 1000: 0.655911\n",
      "Coste tras la iteración 1100: 0.655898\n",
      "Coste tras la iteración 1200: 0.655885\n",
      "Coste tras la iteración 1300: 0.655872\n",
      "Coste tras la iteración 1400: 0.655859\n",
      "Coste tras la iteración 1500: 0.655846\n",
      "Coste tras la iteración 1600: 0.655834\n",
      "Coste tras la iteración 1700: 0.655821\n",
      "Coste tras la iteración 1800: 0.655808\n",
      "Coste tras la iteración 1900: 0.655795\n",
      "Coste tras la iteración 2000: 0.655782\n",
      "Coste tras la iteración 2100: 0.655769\n",
      "Coste tras la iteración 2200: 0.655756\n",
      "Coste tras la iteración 2300: 0.655743\n",
      "Coste tras la iteración 2400: 0.655730\n",
      "Coste tras la iteración 2500: 0.655718\n",
      "Coste tras la iteración 2600: 0.655705\n",
      "Coste tras la iteración 2700: 0.655692\n",
      "Coste tras la iteración 2800: 0.655679\n",
      "Coste tras la iteración 2900: 0.655666\n",
      "Coste tras la iteración 3000: 0.655653\n",
      "Coste tras la iteración 3100: 0.655640\n",
      "Coste tras la iteración 3200: 0.655628\n",
      "Coste tras la iteración 3300: 0.655615\n",
      "Coste tras la iteración 3400: 0.655602\n",
      "Coste tras la iteración 3500: 0.655589\n",
      "Coste tras la iteración 3600: 0.655576\n",
      "Coste tras la iteración 3700: 0.655563\n",
      "Coste tras la iteración 3800: 0.655551\n",
      "Coste tras la iteración 3900: 0.655538\n",
      "Coste tras la iteración 4000: 0.655525\n",
      "Coste tras la iteración 4100: 0.655512\n",
      "Coste tras la iteración 4200: 0.655499\n",
      "Coste tras la iteración 4300: 0.655486\n",
      "Coste tras la iteración 4400: 0.655474\n",
      "Coste tras la iteración 4500: 0.655461\n",
      "Coste tras la iteración 4600: 0.655448\n",
      "Coste tras la iteración 4700: 0.655435\n",
      "Coste tras la iteración 4800: 0.655422\n",
      "Coste tras la iteración 4900: 0.655410\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 5000, tasa = 0.00000001, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida esperada**: \n",
    "\n",
    "<table style=\"width:50%\"> \n",
    "<tr>\n",
    "<td> Coste tras la iteración 0   </td> \n",
    "<td> 0.693147 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "<td> <center> $\\vdots$ </center> </td> \n",
    "</tr>  \n",
    "<tr>\n",
    "<td> Precisión de entrenamiento  </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> Precisión de prueba </td> \n",
    "<td> 70.0 % </td>\n",
    "</tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La precisión de entrenamiento es muy similar a la que conseguimos mediante la regresion logistica. También podemos observar que el error de prueba es igual al de entrenamiento. Este resultado sugiere que el modelo aprende segun entrenamiento, y generaliza de igual forma sobre los observaciones nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafiquemos la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gráfica de la curva de aprendizaje (con costes)\n",
    "costes = np.squeeze(d['Costes'])\n",
    "plt.plot(costes)\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "plt.title(\"Tasa de aprendizaje =\" + str(d[\"Tasa de aprendizaje\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretación**:\n",
    "Se puede ver el coste decreciendo, demostrando que los parámetros están siendo aprendidos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos un primer modelo de clasificación. Ahora examinemos distintos valores para la tasa de aprendizaje $\\alpha$. \n",
    "\n",
    "#### Selección de la tasa de aprendizaje ####\n",
    "\n",
    "Para que el método del GD funcione de manera adecuada, se debe elegir la tasa de aprendiazaje de manera acertada. Esta tasa $\\alpha$  determina qué tan rápido se actualizan los parámetros. Si la tasa es muy grande se puede \"sobrepasar\" el valor óptimo. Y de manera similar, si es muy pequeña se van a necesitar muchas iteraciones para converger a los mejores valores. Por ello la importancia de tener una tase de aprensizaje bien afinada.  \n",
    "\n",
    "Ahora, comparemos la curva de aprendizaje de nuestro modelo con distintas elecciones para $\\alpha$. Ejecute el código abajo. También puede intentar con valores distintos a los tres que estamos utilizando abajo para `tasas` y analize los resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tasa de aprendizaje es: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Sergio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 0.001\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 9e-05\n",
      "Accuracy de entrenamiento: 32.66666666666667 %\n",
      "Accuracy de prueba: 32.75 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1e-06\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1e-07\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 1e-10\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "La tasa de aprendizaje es: 2e-20\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeVzVVf748dfhXi7cCxdxFxVXwBVcQFFAxd00R2vMXJqaSi37Nu06ak2LaYtTNmX+msxpm1wqa9TMNPd9A/cld1NUXFH2/f3743MhRFBQLiic5+PBI++55/P5nM/36/jmbO+jRARN0zRNKyqXsm6ApmmadnfRgUPTNE0rFh04NE3TtGLRgUPTNE0rFh04NE3TtGLRgUPTNE0rFqcGDqVUH6XUQaXUEaXUuELqDFZK7VdK7VNKzc5T/q5Saq/j58E85V8qpY4rpXY6flo78x00TdO0a5mddWOllAmYDvQEYoBtSqmFIrI/Tx1/YDwQLiJxSqkajvJ+QFugNeAGrFFK/SIi8Y5Lx4jIPGe1XdM0TSucM3sc7YEjInJMRNKBucCAfHVGAtNFJA5ARM47ypsDa0QkU0SSgF1AHye2VdM0TSsip/U4gDrAqTyfY4DQfHUCAJRSGwAT8LqILMEIFK8ppaYCNqArsD/PdZOVUq8CK4BxIpJ2o4ZUq1ZNGjRocBuvommaVvFER0dfFJHq+cudGThUAWX585uYAX8gEqgLrFNKtRSRX5VS7YCNwAVgE5DpuGY8EAtYgBnA34GJ1z1cqVHAKIB69eoRFRV1u++jaZpWoSilfi+o3JlDVTGAb57PdYEzBdRZICIZInIcOIgRSBCRySLSWkR6YgShw47ys2JIA77AGBK7jojMEJEQEQmpXv26gKlpmqbdImcGjm2Av1KqoVLKAgwBFuarMx9jGAqlVDWMoatjSimTUqqqozwICAJ+dXz2cfxXAQOBvU58B03TNC0fpw1ViUimUuppYCnG/MXnIrJPKTURiBKRhY7veiml9gNZGKulLiml3DGGrQDigYdEJGeoapZSqjpGL2Qn8KSz3kHTNE27nqoIadVDQkJEz3FoBUlPT+fIkSOkpKSUdVPKhNVqxc/PD4vFUtZN0e5ASqloEQnJX+7MyXFNu+MdOXIEs9mMj48Pjh5uhSEiJCYmcvjwYVq0aFHWzdHuIjrliFahpaSk4OnpWeGCBoBSCk9PT1JSUjh37lxZN0e7i+jAoVV4FTFo5FBKoZTihx9+IDs7u6ybo90l9FDVDex65UfcMt2o0tD35pW1u9N1o7cV0+Udp4n5JBp3s1tZN0UrQZbaHnj3b1zi99U9jkKICFfiLpKccoX4C+e4fu+ippWMVatWERERQVhYGNOmTbvu+7S0NJ544gnCwsLo168fp04ZCRkuX77MoEGD8PPzY8KECbfdjoqwUEYrGbrHUQilFEneVdl1fD0ZsYdpEtaZPqOfw6xXn5Qrp6Kjy/T5WVlZTJgwgblz5+Lj40Pfvn3p3bs3AQEBuXXmzJmDt7c3GzduZP78+UyaNIlPP/0Ud3d3xowZw8GDB/ntt99uqx3uTSpT/bGW2Gy2230lrQLQPY4bsNepgcnzHtqFRnBw41q+e3MCyVevlHWztHJkx44dNGjQgPr162OxWBgwYABLly69ps7SpUt54IEHALj33ntZv349IoLNZiM0NBQ3Nz28pJUu3eO4AXvD2rD7dxrhRa0XxvPLx1OZ9fKL3Pf3V6nmW7+sm6eVsA/WnOLwhZLdz+Ff3crzXQqfI4uNjaV27dq5n318fNi+fXuhdcxmM15eXly+fJmqVauWaFs1rah0j+MGbNU8ALi6/ygBoeE8+NrbZGWkM+cfYzixs2yHOLTyoaB5hfyrvIpSR9NKk+5x3IDVbsxnJBw/g4hQyy+AYZOnMn/KRH589w26PfokrXv1LeNWaiXlRj0DZ/Hx8eHMmT9yf549e5ZatWoVWKd27dpkZmYSHx9P5cqVS7upmpZL9zhuwOYIHGnpiozfjezCXtWqM+SNd2nYOpgV//l/rPpyBtnZWWXZTO0u1rp1a44fP87JkydJT09nwYIF9OrV65o6vXr14vvvvwdg0aJFRERE6B6HVqZ0j+MGcnoc6a52UnbvxuI4DMpitTFgzCus/eZzon9ewJVzZ+n3zBgsVr0iRSses9nM5MmTGTZsGFlZWQwZMoQmTZowZcoUWrVqRe/evRk6dCjPPPMMYWFheHt788knn+Re3759exITE0lPT2fp0qXMmTPnmhVZmuYMOnDcgLuHGRRkeFQhZdduKv3pT7nfubiYiHx4JJV96rDi838z99WxDPz7q3hVq1GGLdbuRt27d6d79+7XlI0dOzb3z+7u7syYMaPAa7du3erUtmlaQfRQ1Q24mFxw93Alu4YvKXv2FFinVc++3D/uda5eOM/sl18k9sihUm6lpmla6dKB4yasdguZ3jVJO3CA7PT0Aus0aNWWYZPew2yx8O0b4zm0ZUMpt1LTNK306MBxEza7Kxnu3khGBmkHDhRar2rdegybPJUaDRrx09S32fK/73QKB03TyiWnBg6lVB+l1EGl1BGl1LhC6gxWSu1XSu1TSs3OU/6uUmqv4+fBPOUNlVJblFKHlVLfOo6ldRqr3UIaxs7clN0FD1flsHlV4oF/TKZpeBfWz/2apZ98SFZmhjObp2maVuqcFjiUUiZgOnAP0BwYqpRqnq+OPzAeCBeRFsBzjvJ+QFugNRAKjFFKeTkuexf4QET8gTjgcWe9AxiBIzU5G3P16qTs3n3T+maLhb5/e4mwB4azb81yvn/zFZKuxDmziZqmaaXKmT2O9sARETkmIunAXGBAvjojgekiEgcgIucd5c2BNSKSKSJJwC6gjzIWr3cD5jnqfQUMdOI7YLW7kpaSiSWoFSm7dxXpGqUUHQcNpd8zYzh3/AjfjHuW0wcLH+bSNE27mzgzcNQBTuX5HOMoyysACFBKbVBKbVZK9XGU7wLuUUrZlFLVgK6AL1AVuCIimTe4Z4nK2cuhmrUh4/eTZF0pepLDpuFdGDbpfcwWN757Yxw7lvyk5z2069xqWnWAadOmERYWRkREBKtXr84tf/755wkMDKRr166l8QpaBePMwFHQ1tb8/2qaAX8gEhgKzFRKeYvIr8BiYCMwB9gEZBbxnsbDlRqllIpSSkVduHDh1t6AP3aP07ApQKHLcgtTvV4Dhr/9AQ1aB7Pyi0/5ZfpUMtJSb7k9WvmSk1Z91qxZrF69mgULFnDo0LVLuvOmVR85ciSTJk0C4NChQyxYsIBVq1Yxe/Zsxo8fT1aWkcXgwQcfZNasWaX+PlrF4MzAEYPRS8hRFzhTQJ0FIpIhIseBgxiBBBGZLCKtRaQnRsA4DFwEvJVS5hvcE8f1M0QkRERCqlevfssvYbW7ApBdqx4oRcqum89z5Ofu4cnAl14h/MG/cGD9aua88hJXYs/ecpu08uN20qovXbqUAQMG4ObmRr169WjQoAE7duwAoEOHDjqfleY0ztw5vg3wV0o1BE4DQ4Bh+erMx+hpfOkYkgoAjjkm1r1F5JJSKggIAn4VEVFKrQIGYcyZPAIscOI75A5VpWWYcPNrTMqe4gcOAOXiQof7H6RWIz9+nvYe34x/jnuefpHGwe1LsrnabfDa8BbmSyU7F5VZtRnx4YWfznc7adXPnj1LcHDwNdfGxsaWaPs1rSBO63E45iGeBpYCB4DvRGSfUmqiUiond8dS4JJSaj+wChgjIpcAV2Cdo3wG8FCeeY2/Ay8opY5gzHn8x1nvAGD1MgJHckIG7oFBpO7afVvzFA1aB/PQ2x9QqUYt5k+ZyIbvZukkiRXY7aRV1+nWtbLi1FxVIrIYY64ib9mref4swAuOn7x1UjFWVhV0z2MYK7ZKhcXdhItZkZKQjjUoiKs//kjGqVNY6tW75XtWqlGLIW9OYcXMT9j8wxxijx6i799ewuppL8GWa8V1o56Bs9xOWvXatWtfd23NmjVLre1axaV3jt+EUgqb3WIEjlZBwM03AhaFq8WN3qOfpceI/+Pknl3MGv8c544fve37aneX20mr3qtXLxYsWEBaWhonT57k+PHjtGnTpixeQ6tgdOAoAqvdQkpCBm7+/ih39yLv57gZpRStet7DkDfeJSsri7n/GMO+NStK5N7a3SFvWvUuXbrQv3//3LTqOZPkQ4cOJS4ujrCwMGbMmMGECUbPqEmTJvTv35/IyEiGDRvGW2+9hclkAmD06NH079+fo0ePEhwczOzZswttg6YVl6oI+wpCQkIkKirqlq//adpOUhMzeGB8O04MfwgyM2nw7dwSbCEkX73Cog+ncGrfblr17EvkIyMxu7qW6DO060VHR18zOV0RnTlzhlWrVvHUU09hs+kzZbQ/KKWiRSQkf7nucRSB1W4hOcHIjGsNCiL1wAGkkEy5t8pWyZtBL79Juz/9mV3LFvPd6+NIuHSxRJ+haZpWEnTgKIKcoSoRwRoUiKSnk3qw5M/dcDGZ6Dz8Ufq/MJ6LMSf577hnObn31pb/apqmOYsOHEVgtbuSlZFNRloW1qCcCfKSmecoSEBoOMPfmorV0873k15mw7f/JTtLL9nVNO3OoANHEeSkHUlJSMdcuzamatVILYGVVTdStY4vw9/+gBZdurP5x2+Z+/rfuXr+nFOfqWmaVhQ6cBSBNTdwZKCUwhoYWKQU67fL4m6lz+jn6PfMGC6dOsnXY//GbxvWOP25mqZpN6IDRxHk5KtKjndMkLcKIv34cbKuXi2V5zcN78LDUz6ial1ffv7onyz5f/8iPTWlVJ6taZqWnw4cRWDz+mOoCvhjnmPv3lJrQ6UatXjw9XcJve9B9q1dwTfjnuXcsSOl9nzNeWbOnEnXrl2JjIzks88+K9a1u3fvplu3boSFhfHKK6/kpiF57733aNu2LT169KBHjx6sWKH3B2klRweOIrB6/jFUBeDesiUAqaUwXJWXyWwmYshfGPyPyWSkpTH7lZeI+ulHJDu7VNuhlZzffvuNWbNm8fPPP7N8+XKWLVvGsWPHinz9uHHjmDJlChs2bOD48eOsWrUq97uRI0eyfPlyli9fTvfu3Z3RfK2C0oGjCEyuLlis5tweh8nLC0ujRreUYr0k+LYI4uEp02jUNoQ133zOD2+/po+nvUsdPnyYtm3bYrPZMJvNdOzYkV9++YUTJ04wbNgwevfuzcCBAzl8+PB11547d46EhARCQkJQSjFo0CCWLFlSBm+hVTROTXJYnljtrrmBA4zhqsR16xCRMslIarV78acXX2b38l9Y/dVMvh77N/qMfo6Gba7b5KkV0Se/fcLRhJLNF9bY3pjRTUcX+n3Tpk159913uXz5Mu7u7qxcuZKgoCDGjh3LO++8Q6NGjdi+fTsTJkzIzVeVIzY2Fh8fn9zPtWvXviat+hdffMG8efMICgritddew9vbu0TfTau4dI+jiGx2C8mOoSowJsizLl0i43SB50iVCiPXVV+Gv/0Btkre/PjO66z++jMyMzJufrF2R/D39+epp55iyJAhDB8+nObNm2M2m4mKimLUqFH06NGDsWPHcu7c9Uuxb5Qu6JFHHmHTpk0sW7aMmjVr8sYbbzjzNbQKRvc4ishqt3DlfHLuZ/dAY4I8dfcuLHWdeuz5TVXzrc+wye+z9psviP55Aaf27aXfs2OoUrtumbbrbnOjnoEzDRs2jGHDjDPO3n77bapXr46XlxfLly+/pl5WVha9e/cGjIy5jzzyCGfP/nGS5JkzZ3JTsuc99XL48OE8/PDDzn4NrQLRPY4iyj9U5d4kAOXmViIp1kuCq8WN7o89yYAx/yD+0gX+O+5Z9qz89bYOndJKx8WLRk6ymJgYFi9ezKBBg/D19eWnn34CjJ7Fvn37MJlMuZPdY8eOpWbNmnh6ehIdHY2IMG/evNzAkreH8ssvv9CkSZPSfzGt3HJqj0Mp1Qf4EDABM0XknQLqDAZeBwTYJSLDHOVTgH4YwW0Z8Kzj6NjVgA+Qs5Ghl4icd+Z7gNHjSE3MIDtbcHFRKFdX3Js3L5WNgMXhFxJKzSkf8cvHU/n10484sTOa7iOewuZVqaybphVixIgRxMXF4erqyltvvYW3tzfTp09n3LhxfPjhh2RkZDBgwABatGhx3bXvvPMOzz33HKmpqXTt2pVu3boBMGnSJPbt24dSirp16zJlypTSfi2tHHNa4HCcGz4d6AnEANuUUgtFZH+eOv7AeCBcROKUUjUc5WFAOMZZ4wDrgS7Aasfn4SJy63nSb4HVbkEE0pIycneSW4MCiZv7LZKRgbqDUqDbq1Rj0Ctvsm3hj2z8bhanDuylx4inCAgNL+umaQWYP3/+dWX16tUr0hkarVq1umYJbo5p06aVSNs0rSDOHKpqDxwRkWMikg7MBQbkqzMSmC4icQB5eg4CuAMWwA3jDPIyTdSUu3s873BVUBCSlkZaAUsly5qLi4nQgQ/w0Dv/wl61Gj9NfZtF/3qX5PjS2e2uaVr55czAUQc4ledzjKMsrwAgQCm1QSm12TG0hYhsAlYBZx0/S0XkQJ7rvlBK7VRK/UOV0lpYm/3aTYAA1latjLI7bLgqr+r1GjBs0vuED36Iw1s38dVL/8fhLRvLulmapt3FnBk4CvoHPf9MrRnwByKBocBMpZS3UsoPaAbUxQg23ZRSnR3XDBeRQKCT4+cvBT5cqVFKqSilVNSFCxdu+2Ws9mvTjgC41qmDqXLlMtsIWFQms5kOfx7CQ29/gGflqiyc+haLPpyiex+apt0SZwaOGMA3z+e6QP5NDzHAAhHJEJHjwEGMQHIfsFlEEkUkEfgF6AAgIqcd/00AZmMMiV1HRGaISIiIhORdmnirrF7GUFXewKGUwhoURMqeOztw5KhevyHDJr9P2ODhHN6y0eh9bNW9D03TiseZgWMb4K+UaqiUsgBDgIX56swHugIopaphDF0dA04CXZRSZqWUK8bE+AHH52qO+q7AvUCpZBp0t7mi1LVDVQDurYJIP3qMrMTE0mjGbTOZzXT889A/eh/vv8XPH/2TlIT4sm6apml3CacFDhHJBJ4GlgIHgO9EZJ9SaqJS6k+OakuBS0qp/RhzGmNE5BIwDzgK7AF2YSzT/QljonypUmo3sBM4DRQvnegtUi4K9zxnj+ewBgaBCKl77oz9HEWV2/t4YDiHNq/nyxef4vC2TWXdLE3T7gJO3QAoIotFJEBEGovIZEfZqyKy0PFnEZEXRKS5iASKyFxHeZaIPCEizRzfveAoTxKRYBEJEpEWIvKsiJTamao2uysp8fkCR1AgwB2zEbA4TGYzHQcNZfhbH+BRuQoL35usex9l4PnnnycwMJCuXbsW+9rC0qoD/Oc//yEiIoLIyEjefPPNkmyyVsHpnePFYLVbrhuqMlWqhKVBgzt6ZdXN1GjQiOGTp9Jx0LDc3seRbZvLulkVxoMPPsisWbNu6drC0qpv2LCBpUuXsmLFClavXs3o0WWTTkUrn3TgKAYjcKRfV+4eFEjK7l13dXoPk9lM2APDcnsfC96bxOKP3yclMaGsm1budejQgcqVK19Tdrtp1b/++muefvpp3NzcAKhWrZrzX0SrMHSSw2LIn68qtzyoFfELfyIzNhbXPGmu70ZG7+N9tvzvO7b87zt+372DyIdH0DS8S5mkjy9NyR9/TNaRkk2rbvJrjO3pp4t93e2mVT969Chbtmzh3Xffxc3NjVdffZXWrVvf3stomoMOHMVgtVtIT80iMyMLs6vpj/JWjqNkd+2+6wMHgMnsStgDw/Fr15FlM6axeNp77F21jO6PP0WV2mWbCbgiSEpKyk2rniM9/fpfWG7Uw83KyuLq1assWrSInTt38sQTT7B58+ZyH/y10qEDRzHk3T1ur/JH4HBr0gTl6krK7t149eldVs0rcTUaNGLopPfYvXwp6+d8xddj/o/2Ax+g/YAHMFssZd28EncrPQNnyM7Ovu206j4+PvTt2xelFG3atMHFxYXLly9TtWrV0nsRrdzScxzFkJOvKv9wlYvFglvzZqV+BnlpcHEx0bpXXx794N8EdIhg07w5fDXm/zixe0dZN63cstvtt51WvU+fPqxfvx4whq3S09OpUqVKmb2TVr7owFEMOWlHkuMLnudI2bcPycws7WaVCg/vyvT920sMemUSSil+mPwPFn04hcS4y2XdtLve6NGj6d+/P0ePHiU4OJjZs2czffp05syZQ48ePYiMjGTp0qUFXvvOO+/w0ksvERYWRv369XPTqg8ZMoSTJ0/StWtXRo8ezYcffqiHqbQSo4eqisFaQKLD3O+CAon7739JO3IE96ZNS7tppaZ+YGsenvIx2xb+wJb533F8RxQRQx+mVc97cHEx3fwG2nU++eSTAstvJ626xWLh448/vu22aVpBdI+jGAobqgKwBjkmyMvhcFV+ZouFjoOG8sg/P8bHvwkrP/83s19+iXPHjpR10zRNKwU6cBSDq5sJs6tLgYHDtV49TJUqVYjAkaOyTx3+PGEi/Z4ZQ+Lli8ya8AIrv/iUtOSksm6apmlOpIeqikEpVeDu8Zzv3IOCSL3DU6yXNKUUTcO70KB1MBu+/S87li7i0JYNdH1kJAEdIvS4uqaVQ7rHUUyFbQIEY7gq7cgRshIr3m/c7h6edH9sNMMnvY+Hd2UW/etdfnz7Na7Enr35xZqm3VV04Cgmq9f1GXJzv2vlyJS7b18pt+rOUcsvgOFvTaXrX5/gzKEDfPniaNbO/pK05OSybpqmaSVEB45iKmyoCsA9MCdT7q7SbNIdx8XFRNt7+vPo1H/TNLwL2xbM4/PnRrF7xVKys0stmbGmaU6iA0cx2RxDVQWlezBXroxrvXqk3oUp1p3Bs0pV+jz1PMMnT8W7Vm2WzZjGN+Oe4+TeijUPdDPOSKv+xBNP0KNHD3r06EH79u3p0aNHSTdbq8B04Cgmq91CdpaQnlLwRj9rUFCFWllVFLX8Ahjyxrvc+9zfSUtO4vs3J7Dgvcl6/sPBGWnVP/3009xd5v369aNv374l2WStgnNq4FBK9VFKHVRKHVFKjSukzmCl1H6l1D6l1Ow85VMcZQeUUh8px/IcpVSwUmqP45655aXlRpsAwdgImHnuHBnnzpVms+54SimadOzEX6d+QsSQh/l99w6+eGE0a775vMIv33VGWvUcIsLChQsZOHCgU99Bq1icthxXKWUCpgM9gRhgm1JqoYjsz1PHHxgPhItInFKqhqM8DAgHghxV12OcO74a+AQYBWwGFgN9gF+c9R755WwCTE5Ix7um7frv82wEdO3Zs7SadddwtbgRet9gWkT2YP3cr4la9D/2rVlB+OCHCOzeq0x3n+9aEsvV2LQSvWelWm606lOr2Nfdblr1HFu2bKF69eo0atTo1l5A0wrgzH0c7YEjInIMQCk1FxgA7M9TZyQwXUTiAETkvKNcAHfAAijAFTinlPIBvERkk+OeXwMDKdXAkdPjKHhllVuzZuDqSuru3XjpwFEoz8pV6DP6Odr0vpdVX33G8pnT2fnrz0Q+PIL6gRX73IiSSKueY/78+bq3oZU4ZwaOOsCpPJ9jgNB8dQIAlFIbABPwuogsEZFNSqlVwFmMwPGxiBxQSoU47pP3nqV6QITtJkNVLm5uuDdpQkoF2wh4q2o28uPB19/h8JYNrPnmC+ZNeoXGIaF0eegxKvuU7tkft9IzcIaSSKsOkJmZyeLFi68bvtK02+XMwFHQ3EP+X5HMgD8QCdQF1imlWgLVgGaOMoBlSqnOQEoR7mk8XKlRGENa1KtXr7htL5S7Z+H5qnJYg4K4On8+kpWFMunEfzejlCKgQwSN2rYnevECtvzvO7588f9o0+deOvx5CO4enmXdxFKVN616//79ERH2799PixYtrgsmOWnV27Zty7x583jsscdyv1u3bh1+fn7Url27tF9BK+ecOTkeA/jm+VwXOFNAnQUikiEix4GDGIHkPmCziCSKSCLGUFQHR/26N7knACIyQ0RCRCSkevXqJfJCACazC242MykFpFbPYW0VRHZyMmlHS/YY0vLObLEQOvABHv9wBs07dyN68QL+88xIti38gYz0kp17uJM4I606wIIFC/QwleYUzuxxbAP8lVINgdPAEGBYvjrzgaHAl0qpahhDV8eARsBIpdTbGD2XLsC/ROSsUipBKdUB2AI8DExz4jsUyGq3kFzIUBWAe6AxQZ66ezfuAQGl1axyw8O7Mr2ffIbWvfuxfs5XrJ31BdsXL6DDn4fSsmtPTObylWLNGWnVAf71r3/dVrs0rTBO63GISCbwNLAUOAB8JyL7lFITlVJ/clRbClxSSu0HVgFjROQSMA84CuwBdgG7ROQnxzWjgZnAEUedUpsYz3GjfFUAlgb1cfHyIkVvBLwtNRs25s8TJjL4tbexV6/B8pnT+fLF0RzYsAbJzi7r5mlaheXUX91EZDHGktm8Za/m+bMALzh+8tbJAp4o5J5RQMsSb2wx2OwWLp8tfO+BcnHBGhioNwKWEN/mgQyd+E+Obd/Ghrlfs/ijf7JtwTwihjxMwzYhOgOvppUyvXP8FtwoX1UO96BA0g4dIlsn9ysRSikaB7fnL+9+RN+/vURGair/e/cN5r72d2IO7C3r5mlahaIDxy2w2l1JTcogO6vw4RJrUBBkZ5O6f3+hdbTiUy4uNIuI5K9TP6HHiP8j/nws374+jh/efo1zx/ViBE0rDTpw3ILcTYCJhfc6cneQ6/0cTmEym2nV8x4e++gzOg9/lNgjh/hm3LP89K93uXzmdFk3T9PKNR04bsHN8lUBmKtWxbVOHRLXryPr6tXSalqF42pxo92f/syIaTPp8OchHN++jS9fHM2vn35E/MULZd08TSuXdOC4BTavm28CBLD36U3yps0cjujEqaefJn7JErJTU0ujiRWOm82D8MEPMWLaTNr0vpf9a1fy+XOjWPXVZyTGXS7r5t3Q7aRVf+eddwgODsbPz++a8rS0NJ544gnCwsLo168fp06dKuQOmlZ8OnDcgpvlq8pR46WXaPD991QeNozUXbs5/dzzHA6P4Mzfx5G4bj2SWXBqdu3W2Sp50/Wvo3jswxk0i4hkx5KfmGCJF+sAACAASURBVPm3x1nx+SfEXzx/8xuUgdtJq96zZ08WL158XfmcOXPw9vZm48aNjBw5kkmTJt1uMzUtV/naSVVKijJUBcZKIGtgS6yBLakxdgzJ27ZxddEiEpb+ytUFCzBVrYpXnz543dsPa+vWellpCfKqVoPeTz5L6H0PsnXB9+xevpTdy5fSoks32g8cjHfNOyMvFRhp1fP3CE6cOMGECRO4dOkSVquVf/7zn/j7+193bXBwcIH3XLp0KS+++CIA9957Ly+//DIiov+OaSVCB45b4GY14+KiCj17vCDKZMKjQwc8OnQg+9VXSVq7lquLfubKvHnEzZqFa926ePXrR6V7++FWwD8Q2q3xrlmLXqP+Rof7H2Tbwh/Zs3Ipe1cvp1l4F9rfN/iautE/ziXudMkO6VSu40vw/UOKfV1R0qrfSGxsbG6OKrPZjJeXF5cvX6Zq1arFboum5acDxy1QLgr3m+wevxEXiwV7jx7Ye/QgKzGRhOXLiV/0M5c++4xLn36KW5MmeN3bj0p9++Jap3QzxJZXXtVq0P2xJwm9bzBRi/7HrmWL2b9+NZFjXi/rpl2nqGnVb6SglOu6t6GVFB04blFRNgEWhcnTE++BA/EeOJDMixeJX7KU+EWLuPD+VC68PxVr27Z49e2LV+9emEswWWNF5Vm5CpF/eZz2AwYR/fP8a767lZ6BMxQ1rfrYsWMLvYePjw9nzpyhdu3aZGZmEh8ff90pg5p2q/Tk+C2y3UaPozDmatWo8tBwGsydQ+Nlv1L9uWfJTkjg3KRJHO4Sye9/fZS4b78jMy6uRJ9bEdm8KtFp6CNl3YwC5U2rDkbvYd++fZhMptxzxG8UNMAILDlDW4sWLSIiIkL3OLQSowPHLTJ6HCUbOPKy+PpS7cknafTTQhr9tJBqTz5BZmwssa+9xuFOnTk5chRX/jefrIQEp7VBKx23k1b9zTffJDg4mJSUFIKDg3nvvfcAGDp0KHFxcYSFhTFjxgwmTJhQmq+klXOqKMdP3u1CQkIkKiqqRO+5/vvD7Ft/hic+7FKi970RESHtt9+IX7yY+MW/kHH6NMrVFY9OnfDq2xd710hcPDxKrT3lQXR0dIU/6OjMmTOsWrWKp556CpvNVtbN0e4gSqloEQnJX67nOG6R1e5KZloWGWlZuLqVzil/SincmzXDvVkzqr/wAql79hD/82LilywhceVKlLs7npGReN1zD55dOuPi7l4q7dI0rWLRgeMW5d0E6OpmLfXnK6WwBgVhDQqixt/HkrJ9O/GLfyF+6VISlizBxWbDs3t3vO65B4+IcFwsllJvo6Zp5ZMOHLfIlmcToFe10g8ceSkXF2whIdhCQqg5YTzJ27YRv/gXEn79lfiffsLF0xPPbl3x6t0bj4gIXNzcyrS9mqbd3Zw6Oa6U6qOUOqiUOqKUGldIncFKqf1KqX1KqdmOsq5KqZ15flKVUgMd332plDqe57vWznyHwhQ17UhpU2YzHh074vPmRPzXr8P3s8+w9+lN0pq1xPzf0xwOC+f0S2NIWL6c7LTye463pmnOU+Qeh1LKCtQTkYNFrG8CpgM9gRhgm1JqoYjsz1PHHxgPhItInFKqBoCIrAJaO+pUwTgm9tc8tx8jIvOK2nZnsNqNRIfF2T1e2pSrK56dIvDsFIG89hpJW7aSsHQJCcuWE79okTGc1bUr9j698ezUSc+JaJpWJEUKHEqp/sB7gAVo6Pgtf6KI/OkGl7UHjojIMcc95gIDgLwnG40EpotIHICIFJSFbhDwi4jcUUfp3ak9jsIoV1c8I8LxjAin1quvkrR1KwlLlhq71n/+2QgikZHYe/fGs3MnXKxlO/ymadqdq6hDVa9jBIIrACKyE2hwk2vqAHkT/8Q4yvIKAAKUUhuUUpuVUn0KuM8QYE6+sslKqd1KqQ+UUmUyYO/qZsLsZiqR3eOlTbm64hkebgxnrVtLvc//g1f//iRt3szpZ5/lUHgEMc8/T/ySpWSnpJR1c8u906dPM2jQIDp37kxkZCQzZ84s8rXJycn85S9/oVOnTkRGRjJ58uTc73Rqdc1ZijpUlSkiV4u587Sgyvk3jZgBfyASqAusU0q1FJErAEopHyAQyLv7aTwQi9H7mQH8HZh43cOVGgWMAqhXr15x2l1kztg9XtqU2YxHWBgeYWHU+scrJEdFEb/EGM5K+GWJscS3UyfsvXriGRmJyW4v6yaXO2azmVdffZWgoCASExPp06cPnTt3JiAgoEjXP/nkk4SHh5Oens7gwYNZuXIl3bp1uya1+vz585k0aRKffvqpk99GqwiK2uPYq5QaBpiUUv5KqWnAxptcEwP45vlcFzhTQJ0FIpIhIseBgxiBJMdg4H8ikvtrvYicFUMa8AVGT+g6IjJDREJEJKS6k3I8OXv3eGlTZjMeHTrg8/rr+K9dQ70vv8T7/vtJ2bmTM2PGcigsnJOjRnFl3jwyL9/ZhyPdTWrWrEmQ46hhT09P/Pz8OHv2LCdOnGDYsGH07t2bgQMHcvjw4euutdlshIeHA2CxWAgMDOTs2bOAkVr9gQceAIzU6uvXry8w+aGmFVdRexx/A14G0oDZGD2AN29yzTbAXynVEDiNMeQ0LF+d+cBQ4EulVDWMoatjeb4fitHDyKWU8hGRs8ro/gwE9hbxHUqc1W4h4XL5PNHPSAMfikeHUGq+8jIpu3aR8OsyEn79lbOv/ANcXsPWrh32nj2x9+yBa82aZd3k25a95gJyoWRXmqnqbrh0KfovLqdOnWLv3r20bduWxx9/vFip1a9evcqyZcsYMWIEoFOra85T1MDRT0RexggeACilHgAK/VssIplKqacxgowJ+FxE9imlJgJRIrLQ8V0vpdR+IAtjtdQlx/0bYPRY1uS79SylVHWMobCdwJNFfIcSZ7W7cv73+LJ6fKlRLi7Y2rTB1qYNNcaOIe3AAeKXLSPh12WcmzSJc5MmYW3VCnuvnth79cLi63vzm2rXSUpKYsSIEUycOBEXF5dipVbPzMzkqaee4vHHH6d+/fqATq2uOU9RA8d4rg8SBZVdQ0QWA4vzlb2a588CvOD4yX/tCa6fTEdEuhWxzU5ntVtITchAsgXlUjH+B6mUwr15c9ybN6fGs8+SdvSoMR/y66+c/+d7nP/ne7g1bYq9V0+8evbE4ud31/xjVZyeQUnLyMhgxIgR3H///fTt25eEhIRipVYfM2YMDRs2ZOTIkbl1dWp1zVluGDiUUvcAfYE6SqmP8nzlBVT4A7NtdgvZ2UJaSibuHq5l3Zwy4da4MW6NG1PtySdIjzlNwrJlJCxbxsVpH3Pxo2lYGjTA3qM7nt27Y23VCuWiEzLnJyK8+OKL+Pv788QTTwDXplbv378/IsL+/ftp0aLFdcHk3XffJSEhgffff/+a8pzU6iEhITq1ulaibtbjOANEAX8CovOUJwDPO6tRd4ucTYApCekVNnDkZalbh6qP/pWqj/6VjPPnSVi+nMTlK7j05VdcmvkfTNWrYe/WHXuP7thCQ3X+LIetW7cyb948mjVrRo8ePQAYP34806dPZ9y4cXz44YdkZGQwYMAAWrRocc21Z86c4cMPP8TPz49evXoB8OijjzJ8+HCGDh3KM888Q1hYGN7e3nzyySel/m5a+XTDwCEiu4BdSqnZOSublFKVAd+cTXsVWd5NgJVr6XTmebnWqEGVYcOoMmwYWfHxJK5ZS8KKFcT/9BNXvv0WFw8PPLt0xrN7dzw7d67Qy3xDQ0M5cyb/gkPD7Nmzb3ht7dq1C73W3d2dGTNm3Hb7NC2/os5xLFNK/clRfydwQSm1RkSum5uoSHICR3L83bcJsDSZvLyo1P9eKvW/l+y0NJI3byZh+QoSVq4kfvEv4OqKR2ioMaTVtRuuNWuUdZM1TbuBog44VxKReOB+4AsRCQZ6OK9Zd4e8Q1Va0bi4ueHZpYuxa33tGurPnk2Vh/9C+qmTxL7+Bke6dOH4gw9yccZnpB07dvMbappW6ora4zA7dnEPJs+S3IrO6qkDx+1QJhO2tm2wtW1DjZdeIv3oURKWLydh+QouTJ3KhalTsTRsiGe3rti7dcPaujXKVPKHZolIhZ00FhG9KVArtqIGjokYey42iMg2pVQj4PptrBWMi8kFdw/XuzJf1Z1GKYWbnx9ufn5Ue/JJMs6eJWHlShJXruLy1//l8n8+x+TtjWeXLnh264ZHeDgmz9ufV7JarSQkJGC32ytc8BAREhISyMjQf3+14ilS4BCR78mzZ8OR8fbPzmrU3cRaDvJV3YlcfXyoMnw4VYYPJysxkaT160lYuZKE1au5umABytUVW4cO2Lt1xbNrV1xr1bql5/j5+bFr1y4SEhIqZODIyMjg+PHjiAgueqm0VkRFTateF5gGhGMkKlwPPCsiMU5s213Barfc0WdylAcmT0+8+vTBq08fJDOT5O3bSVy5ioSVK4l9YyK8MRH35s3x7NYNz66RuDdvXuQgYLFYqFy5Mj/88ANeXl6YzeXjUMz0lGQunvydi6d+Jy05ERcXE961fKhStx7eNWrl7qcREeLj46lTpw5u+mRIrYhUUcY3lVLLMHJU/ddR9BAwXER6OrFtJSYkJESioqKccu8lM/Zy6XQiw9/o4JT7a4UTEdKPHSNx1SoSVq4iZccOEMFcqxaeXSOxd+tW5P0ie/fuZceOHaSVs1MRRYSkuMtcijnF5TOnyExLw+TqSpU6dalatz5eVatRy8eHbt26YdVnsGj5KKWiRSQkf3lRf72qLiJf5Pn8pVLquZJp2t3NZnclRvc4yoRSKnfnetURI8i8dInENWtJXLWSq/MXcGXOXJTNhkdYR+yRkXh07oxrjYKX+rZs2ZKWLVuW8huUrqzMTH7fs4MD61ZzJGozmedPkV29Bl4eXUi6eB6rb/2ybqJ2lyhq4LiolHqIPw5UGgpcck6T7i5WLwtpyZlkZWZjMusx4rJkrloV7/vvw/v++8hOSyNp0yYS16whcfUaEpevAMC9RQs8IyPxjOyCe4sWFSoFislsplGbdjRq04701BSObtvM/vWr2bbwB7bO/57q9RvSrFNXmoZ3xl6lWlk3V7uDFXWoqh7wMdARY45jI/CMiJx0bvNKhjOHqvauPc2a2Qf56zvheHjrMeI7kYiQduiQEUBWryZl504QwVStmrF7vUsXPMJKZpXW3SjpShwHN63nwPpVxB45BErh2zyQZhGR+IeG4e7hWdZN1MpIYUNVRQ0cXwHP5aQZUUpVAd4TkcdKvKVO4MzAcWzHBX75dA+DX25Hdd+KmzbjbpIZF0fSunUkrl5N4rr1ZCckGLvX27XDM7ILnpGRWJx0auSdLu7saQ6sX8OB9au4EnsWk9lM/VZtCQgNp3FIqA4iFcztBo4dItLmZmV3KmcGjrNHrvDje9vp/0wr6jXXB+TcbSQjg+QdO4zeyJo1pB89CmBsPIyMxLNLF2xt26AqWEJGESH26CEOblzLoc0bSbh0AReTmfpBrQnoEIFfSAfcPXUQKe9uN3DsAiLz9TjWiEhgibfUCZwZOK6cS2bWa5vp8WhzmoTe2l4C7c6RfvJkbhBJ3roVycjAxcMDj7COeHTujGfnzuXitMPiEBFijxzi4Ob1HN6ygfgL53ExmagX2JqA0HD82nXAavcq62ZqTnC7geNhjIOb5mHMcQwGJovIf29yXR/gQ4wTAGeKyDsF1BkMvO647y4RGaaU6gp8kKdaU2CIiMx3HEU7F6gCbAf+IiI3XNbkzMCRlpLJzOfXEj7Ij9Y9KubwRnmVlZhE8pbNxkqttWvJjI0FwK1JEzw7d8azcycjDYprxUmpLyKcO3qYQ1s2cGjzeq6eP4dycaFey1YEdAjHr11HbF6VyrqZWgm5rcDhuEFzoBvGka0rRGT/TeqbgENATyAG4wzyoXmvU0r5A98B3UQkTilVQ0TO57tPFeAIUFdEkpVS3wE/ishcpdS/MYLNDQ8acGbgEBH+/bfVtO7uS8f7/JzyDK3siQhphw+TtHYtiWvXkbx9O2Rm4mK34xEWhmfnznh0iih0uW95JCKcP36UQ5vXc2jzBq6cO4tyccG3eSABHSLwb98RWyXvsm6mdhtuO3DcwgM7Aq+LSG/H5/EAIvJ2njpTgEMiMvMG9xkFdBGR4crYDnwBqOU40/yaZxTGmYED4KvxG6jbrArdH27mtGdod5ashASSNm4icd1aktasJfPCBQDcmjdz9EY6Yw0KQpWTneg3IyJc+P04hzYbPZG4s6dRyoW6zVrQOKQDjUNC8a6ph3LvNre7AfBW1AFO5fkcA4TmqxMAoJTagDGc9bqILMlXZwgw1fHnqsAVEck5tjaGAs4lL21Wu0Xnq6pgTHY7Xr174dW7l9EbOXgwd0jr0mczufTvT3GpVAnP8DA8IjrhER5ers8ZUUpRo0EjajRoRPiDD3Hx1O8c2ryew1s2svrrz1j99WdU861P45BQGoeEUquRf4XaQ1PeODNwFJQsKH/3xgz4A5FAXWCdUqqliFwBcKRyD8TIzFvUe+K4dhQwCqCek5dWWu2upMTrwFFRKaVwb9oU96ZNqfbEKLKuXiVp40YS164jcf0647AqjLkRj4hwPDt1wtq2bbk9OlcpRfV6DaherwHhgx/iSuxZjkZv4WjUFrYumMeW/32Hh3dlGgW3xy+kA74tg3C16D1QdxNnBo4YwDfP57oYZ5jnr7PZcSztcaXUQYxAss3x/WDgfznH1gIXAW+llNnR6yjongCIyAxgBhhDVSXwPoWy2i3EnU125iO0u4ipUiW87rkHr3vu+aM3sm4dSes35KaIV1YrHqGheERE4NkpAkv98pvuw7uWD8H9BhLcbyApiQkc3xHF0agt/LZhLXtWLMXs5kaDoDY0DulAo7bt9OT6XcCZgWMb4O9YBXUaY8hpWL468zHSl3yplKqGMXSV99i3oRiruQAQEVFKrQIGYaysegRY4LQ3KKKcoaqKfCCQVrBreiMjRxortbZuJWn9OhLXrSdx9WrOAa6+vnh2isAjIgJb+9Byu4vd6mmneaeuNO/UlcyMDGL27eZI9FaORm/hyLbNKOVC7SZNaRxsDGlVqV23rJusFcBpk+MASqm+wL8w5i8+F5HJSqmJQJSILHRMdr8P9AGyMJb4znVc2wDYAPiKSHaeezbij+W4O4CHROSGKU2dPTm+/dff2fTjUUb+qzMW94oxGaqVjPTffydx3XqS1q8naetWJDkZXF2xtWmDR6cIPCMicGvSpNzPB+Ss0DKGtLZy/oSxEbOyTx0atg6mYetg6jRvqYe0Slmpr6q6kzg7cPy26SwrvjrAQ292pFJ1nZpauzXZ6emkbN+eO6yVdvAgAKaqVfHo2BGP8HA8wsLK9SR7jviL5zkavZVj27cRs28PmRnpmC1u+DZvSYPWITRs3ZbKPmW+Lqbc04HDiYHj972XWPTxLv48NphajfT4rFYyMs6dJ2nDBpI2biRp0yayLhkJqd38/fAIC8MjPBxbSAguNlsZt9S5MtLTiNm/l+M7ozixM5q4s8a0pndNHxq0bkvD1iH4tgjE1c29jFta/ujA4cTAcf73eL5/O4q+owNp2Kq6056jVVySnU3awYNGENmwgeSoaCQ9HeXqirVt29zeiHvzZuV+WOtK7FmO74rmxM5oTu7bnXs4Vd1mLWnYOpgGrYOpUruunm8sATpwODFwJFxO5esJG+n6UFOaR9R22nM0LUd2airJUdG5gSR3WKtyZTw6dvhjWMvHp4xb6lyZ6emc/m0/x3dGcXxnNJdPG1vHvKrXMIJIq2B8WwTiZiufiw2cTQcOJwaOzIwsPv3bGkIHNCLkngZOe46mFSbzwgWSNm0iacMGEjduJOvCRQAsjRoZ8yMdO2Br3x6TV/lORhh/4TzHd0ZzfGc0J/fuIiM1BaVcqNXYn3qBrajXshW1A5phLqd7aEqaDhxODBwAnz23hqZhPnQaHODU52jazRgHVx02eiMbN5IcFYWkpICLC+6BLfHo0BGPjh2xtm1TbjchAmRlZnDm0G+c3LuLk3t2cfbIQSQ7G7OrhdpNm1OvZSvqB7amRsNGuLiYyrq5dyQdOJwcOL75xyZqNPCi1+MtnPocTSsuSU8nZdcuo0eyaTMpu3dDVhbK3R1b27Z4hHXE1rEj7s3K9/xIWnIyMQf2OgLJTi6e+h0ANw8PfJsHUS/QCCSVfero+REHHTicHDh+mBKN2eLCgOfuirOttAosKzGR5K3bSNq0ieTNm0g7fAQAk7c3ttBQY2grrCOuvr7l+h/QpCtxnNy3m5N7dnJy7y7iLxiJuT2rVM3tjfi2DKrQ56/rwOHkwLH4k93EX0xhyD/y53HUtDtbxvnzJG/eTNLGTSRt2kTmuXMAuNaujS2sIx6hHbCFti/XKeNFhKvnYjm5dxe/79nJyX27SU2IB4yUKXWbtcz98apeo1wH1Lx04HBy4Fg16zeO77rIY1MinPocTXMmESH9+AmSNm00gsmWrWTHG/+AWho3xiO0PbbQDtjat8NcuXIZt9Z5JDubCydPcHLvLmIO7OP0gb2kJiUCYK9anbrNW1K3WQvqNguksk/tchtIdOBwcuDYsvAY0b+cYPT0riiX8vmXSKt4JCuL1AO/kbxlM0lbthj7R5KTQSncmjbFIzQUW2h7bO3aYSrHZ5BLdjYXY04Ss38PMQf2EXNgL8lXrwBgq+Rt9EaaGz2SanXrlZu5Ih04nBw4dq86xbpvD/PYexFYPcvvShWtYpOMDFL27DUCyeYtpOzYgaSng8mEe8sWeIR2wKNDKNY2bXCxlt/0OyJC3NnTxBzYS8z+vZw6sJfES8YSaHdPO3WatsC3eUvqNG1BjQaNcDHdnau2dOBwcuA4HHWOX2fuY+iroVSprTcbaRVDdloaKTt2krRlM8mbt5CyZw9kZho72lu1wtbBGNaytmqFi1v5TVAoIsRfOJfbG4nZv5cr584CYHZzw6dxALWbNKN2QDN8Appi9bSXcYuLRgcOJweOmINxLPhgBwOfb0OdJuV37FfTbiQ7KYnk7dtJ2mwEktT9+0EEZbFgbd0aW/v2FSKQACRcvsjpA/s4c/g3zhz8jfMnjiLZRqLvKrXr5gaS2gHNqFK7zh05vKUDh5MDx6UzicyduJVeI1rgH1LTqc/StLtFVnw8yVHRJG/dSvLWraQeOGAEEjc3RyBph0f79ri3alWuNyMCZKSmEnvsMGcOHuDMoQOcOfQbqYkJALh7eOIT0DQ3kNTy88fiXvZDfWVx5niFYrMbf+lTEjJuUlPTKg6Tlxf2bl2xd+sKXB9ILn48nYvycYUIJK7u7vg2D8S3eSDwxzxJ3kByfIfxC65ycaF6/YbUDmiKj18Tajb2p4rPndMr0T2OEpKdLfz7/1YRfE8DQv/UyKnP0rTyIuvqVZKjo0nespWkbVtJO/DbtT2Sdu2wtWuHtVUQLu7lP216amIiZw//lhtIzh45REZqCgAWq41ajf2p5ReQ+19nb04skx6HUqoP8CHGCYAzReSdAuoMBl4HBNglIsMc5fWAmRjnlgvQV0ROKKW+BLoAVx23+KuI7HTmexSFi4vC3dOVlIT0sm6Kpt01TJUqYe/WDXu3bsD1geTi9OlGIHF1xT0oCFtIiBFIWrcul8frunt60rBNCA3bGP9WZ2dncfl0DLFHDhF79BCxRw8T9dOPZGdlAeBZuYojkBg/NRv74e7h/GXRTutxKKVMwCGgJxCDcQb5UBHZn6eOP/Ad0E1E4pRSNUTkvOO71RhHyS5TSnkC2SKS7Agci0RkXlHbUho9DoA5E7fgXcPGPU8GOv1ZmlYRZF29SvL27SRHRZG8LYrUffsgK8tY/tu8udEjCQnBFtwWU6WKcYhaZno6508cyw0ksUcOEXf2dO73lX3q5AYTH78AajRshMnsekvPKoseR3vgiIgcczRgLjAA2J+nzkhguojEAeQJGs0Bs4gsc5QnOrGdJcZqt+geh6aVIFOlSti7dsXe1ZgjyU5KInnnTpK3bSM5Koq4//6Xy59/bmxIbNLECCIhIdjahWCuWrWMW+8cZouF2gFNqR3QNLcsNTGR2GOHOXf0MGePHOLknp0cWLcKgIenTKN6/YYl24YSvdu16gCn8nyOAfIncgoAUEptwBjOel1EljjKryilfgQaAsuBcSKS5bhuslLqVWCFozzNea9RdDa7K+dPJpR1MzSt3HLx8MAzPBzP8HDAsY9k1y6So6JIiYriyg8/EPfNN4BxFokRSIKxtg3GtU75TQ3i7ulJg6A2NAgykqyKCImXLxF75BBV69Yr8ec5M3AU9P+h/ONiZsAfiATqAuuUUi0d5Z2ANsBJ4Fvgr8B/gPFALGABZgB/ByZe93ClRgGjAOrVK/n/wxXE6HHoVVWaVlpc3NzwaN8ej/btASOFfOr+/SRHRZG0bRvxixdz5bvvADDXqoUtODg3kLj5+90xq5RKmlIKe9Vq2Ks6Z/LcmYEjBmNiO0dd4EwBdTaLSAZwXCl1ECOQxAA78gxzzQc6AP8RkbOOa9OUUl8ALxX0cBGZgRFYCAkJKZWlY1a7hfSUTLIysjG5ls+/kJp2J8vZaGht3ZqqI0YgWVmkHT5sLAGOjiJ561bif/4ZAJdKlbC1aYM1uC224BCsLVugytkSYGdxZuDYBvgrpRoCp4EhwLB8deYDQ4EvlVLVMIaojgFXgMpKqeoicgHoBkQBKKV8ROSsMvqcA4G9TnyHYrHajQmolMR0PCuX/6WDmnanUyYT7k2b4t60KVUeGo6IkBETQ3JUNCnbo0mOiiZx9Wqjrpsb1sBArCHBRiBp07pcJ268HU4LHCKSqZR6GliKMX/xuYjsU0pNBKJEZKHju15Kqf1AFjBGRC4BKKVeAlY4AkQ08Jnj1rOUUtUxhsJ2Ak866x2Ky5pnE6AOHJp251FKYfH1xeLri/d9AwHIvHSJTNOnkQAAFR5JREFU5O3bSYmKJjk6mkufzeTSvz+F/9/euQfJVdV5/PPtx0xmkjEQCBgDkeAmCEHyMIsCsgJiDIigKyXgC8QS0UWXtWTF1XJZqrbkoa4valdEHrpxxQdEVISwKIuCQUJmJpBAJER3jQESQkJIZjIz3fPbP+7pmTs93cn0TN/umc7vU9XV5557zr2/e/p2f/v8zrm/k0rR/NqjaF24iJZFC2ldtIjsjBl1voLxgT8AWEWe2/gSP7nuMc76xHxePa8xZ3Q4TqPTv3t3GHB/LBKUNWuiUPJAZsaMyL21aBGtixbSfNRRaIJGvh0JHnKkBgy4qnxKruNMWFKTJzP5xBOZfOKJAFgux56n1tO9ejVd7avpeuwxdt5990DZlvnzB4Rk0nHzG/LBxGJcOKrIgKtqp8+scpxGQZkMLcfOo+XYeUz74AcwM3KbN9O1up3u9tV0rW4feMJ9f3FvuXBUkWxzmnQ25T0Ox2lgJJGdOZOpM2cy9R1nAZDftYvujs6BXsmOO+9k+7JlQDQNuGXBAloXRrO9Jh199ISfveXCUUUk0dLm8aocZ38jPWUKU950ElPeFD2YaLkce9avp3t1O90dHXS3t/PyPfcA0eytSfPm0RKEpHXBAjLTp9fT/Ipx4agyrW1NdPlDgI6zX6NMhpZ582iZNw8+8H4A+p7fMiAi3R0dbP/u93jxOzcDkD3sMFoWLqRlwXxaFy6kee5clBm/P8/j17IJSktbE107vcfhOM5QsoceQvZtS3jF25YA0N/by561a+lu76C7o4OulSvZ+bOfAaDW1uiZkgULaFkwn5b588lMm1ZP84fgwlFlWtqybPvLhIjJ6DhOHUk1NdG6cCGtCwfjS+U2b6YrCEl3ezvbbropigYMZGfNimZwhdeko+bWbazEhaPKtLQ10fVyL2bWsAHVHMepPkMG3c96OwD93d1Rr6Szk+6OzqG9ksJYSUFMFswn+8pX1sRWF44q09LWRH/O6N2Tp7nFm9dxnNGTamkZCBUPoVfy3HMDQtLd2cn2Zct48ZZbAMgceugQIZl0zDGkWqq/drn/slWZ1sJDgDt7XTgcx6kqksjOmEF2xgxesXQpECICr18/ICTdnZ28vGJFVCGTYfaPfsiko4+uqh3+y1ZlBuNV9XLAoa11tsZxnEZHTU3RQPrrXjcwgyu3bRvdnWvo7uyk6cgjq35OF44qEw906DiOUw8yBx1E22mn0nbaqYkc3xeNqDIF4ejyhwAdx2lQXDiqjAc6dByn0XHhqDLpTIrm1oy7qhzHaVhcOBIgWnvcexyO4zQmiQqHpKWS1kvaIOnKMmXeI2mdpLWSvh/LnyVphaQnw/4jQv5sSY9IelrS7ZLGXZhJD3ToOE4jk5hwSEoDNwBnAMcAF0g6pqjMHOCzwElmNg+4PLb7u8D1ZnY0cDywJeRfC/ybmc0BtgMfTuoaRkuLBzp0HKeBSbLHcTywwcw2mlkv8APgnKIyHwFuMLPtAGa2BSAITMbM7gv5u8ysK6w/fhrw41D/NuCdCV7DqHBXleM4jUySwjET+HNse1PIizMXmCvpIUkrJS2N5e+QdIekdknXhx7MQcAOM8vt5Zh1p6Uty57dffTn++ttiuM4TtVJUjhKRfizou0MMAc4BbgAuEnSASH/ZODTwF8DRwIXjfCY0cmlSyStkrRq69ato7F/1LS2NYHBnt25fRd2HMeZYCQpHJuAw2PbhwGbS5T5qZn1mdkfgfVEQrIJaA9urhywHFgEvAAcICmzl2MCYGY3mtliM1s8vcara8XDjjiO4zQaSQrHo8CcMAuqCTgfuKuozHLgVABJBxO5qDaGugdKKvzinwasMzMDfg2cG/IvBH6a4DWMisJDgP70uOM4jUhiwhF6CpcB9wJPAj80s7WSrpZ0dih2L7BN0joiQbjCzLaZWZ7ITXW/pMeJXFTfDnU+A3xK0gaiMY/vJHUNo8V7HI7jNDKJBjk0s7uBu4vyvhBLG/Cp8Cquex9wXIn8jUQztsYtrQXh2OlTch3HaTz8yfEEaG7NoJS8x+E4TkPiwpEASomWKf70uOM4jYkLR0L40+OO4zQqLhwJ4fGqHMdpVFw4EsLDjjiO06i4cCREa1uTr8nhOE5D4sKREC2vyNLXk6evN19vUxzHcaqKC0dC+EOAjuM0Ki4cCTEoHO6uchynsXDhSIhCvCrvcTiO02i4cCREq7uqHMdpUFw4EsJdVY7jNCouHAmRbU6TaUp5aHXHcRoOF44E8YcAHcdpRBINq76/09LWxMvb9rDzhW4yTemoF5JNoVSpFXAdx3EmBi4cCdJ2YDPPtG/le5//3ZD8dDZFtilyZUUurZBuGkxnmtNks7F0Ib+pKN08PD+dTSG5ODmOkwyJCoekpcDXgDRwk5ldU6LMe4CrAAM6zey9IT8PPB6K/Z+ZnR3ybwXeDLwU9l1kZh0JXsaoOfm8ubzm9YeQ682T6+2nryc/mO4dnt7TlSO3o4dcb56+3v5of08eswpPLIKQpMhkhwtUsfgM7M/uXZBcnBzHgQSFQ1IauAF4K7AJeFTSXWa2LlZmDvBZ4CQz2y7pkNghus1sQZnDX2FmP07K9mox+YBm5iw+dEzHMDP6cxbEpT+ISizdkyfXN1yYBsQn7O/rifL27Ooj19czWDcIF/UQp6Y0mWYXJ8eZaCTZ4zge2BCWekXSD4BzgHWxMh8BbjCz7QBmtiVBeyYkkkhnRTqbgsnJnGOoOMV6QT0xkerLk+sZ2lMa0jOK9apKiVOutzo9p0wJF1+mKbj1SojQMBdgcZ6POTlOxSQpHDOBP8e2NwFvKCozF0DSQ0TurKvM7J6wb5KkVUAOuMbMlsfq/aukLwD3A1eaWU8SF7C/MFScsomcY189p7IuvJ4ikeqL9vd05di1vWeYiFl/peo0dMxpqLAUtksLULZof3nRSpFK+wRGp3FIUjhK/Y0r/lZngDnAKcBhwG8kHWtmO4BZZrZZ0pHAryQ9bmbPELm2ngOagBuBzwBXDzu5dAlwCcCsWbOqc0XOqKlFzwkgn+8fEJvh40glRCouTH39Q3pOvXvydO3sHazXF9Xpz1UuTqm0RiBAg0JULErlhWywbjrjrj2nNiQpHJuAw2PbhwGbS5RZaWZ9wB8lrScSkkfNbDOAmW2U9ACwEHjGzJ4NdXsk3QJ8utTJzexGImFh8eLFlX/TnQlJOp0i3ZqiuTW5c+Tz/eSHiVFRL6ovuPp6+gfGoAqi1RcTolxvnt0v5Ur0wPorNyzu2hsmQEPfs9kSecU9qFJjV1nvPTnJCsejwBxJs4G/AOcD7y0qsxy4ALhV0sFErquNkg4EusysJ+SfBFwHIGmGmT2r6K/VO4EnErwGxxlGOp0i3ZKiqSW5r4+Zke8bLk7FY0zl8oYKVOTa272jZ8iY02hde6mMot5OtpSrbnjvqVTvKJMd7tYb8p7xsafxTGJ3vpnlJF0G3Es0fnGzma2VdDWwyszuCvuWSFoH5IlmS22TdCLwLUn9RE+3XxObjbVM0nQiV1gHcGlS1+A49ULSwA9vkuTz/WXGmQZn7eX7iidCVNB76on2jYZBYdrL2NNIysTzsnGhc/feaJFVPNVl4rF48WJbtWpVvc1wnP2SQu9prz2mvlK9qMHp5JH7r4Rgxd7zuTG694pn7gXhiSZPlO9dDZ9cMVi/sC+V0YQUKEmPmdni4nx/ctxxnESJ954mkcysPYD+fhvusispSEP3lZo4kevL09OdY9eOnphgRe/9+cr/bCsI1PDeUunp4vHe1PCxp/JuwFS6NgLlwuE4TkOQSommSRmaJiV7nsLkiNyAC2/4+NLAvp78MOEp3o5HjIj3rKolUG//+OuYOr26s0VcOBzHcSpgcHJEsucZOv40vPcUd9sNTqQY3uNKYpzMhcNxHGccUhCo5gRn740Wn5DtOI7jVIQLh+M4jlMRLhyO4zhORbhwOI7jOBXhwuE4juNUhAuH4ziOUxEuHI7jOE5FuHA4juM4FbFfBDmUtBX431FWPxh4oYrmVBu3b2y4fWPD7Rsb492+V5vZ9OLM/UI4xoKkVaWiQ44X3L6x4faNDbdvbIx3+8rhrirHcRynIlw4HMdxnIpw4dg3N9bbgH3g9o0Nt29suH1jY7zbVxIf43Acx3EqwnscjuM4TkW4cAQkLZW0XtIGSVeW2N8s6faw/xFJR9TQtsMl/VrSk5LWSvr7EmVOkfSSpI7w+kKt7Avn/5Okx8O5hy3wroivh/ZbI2lRDW07KtYuHZJ2Srq8qExN20/SzZK2SHoiljdN0n2Sng7vB5ape2Eo87SkC2to3/WSngqf352SDihTd6/3QoL2XSXpL7HP8Mwydff6XU/Qvttjtv1JUkeZuom335gxs/3+BaSBZ4AjgSagEzimqMzHgf8I6fOB22to3wxgUUi3AX8oYd8pwM/r2IZ/Ag7ey/4zgV8CAt4IPFLHz/o5ovnpdWs/4G+ARcATsbzrgCtD+krg2hL1pgEbw/uBIX1gjexbAmRC+tpS9o3kXkjQvquAT4/g89/rdz0p+4r2fxn4Qr3ab6wv73FEHA9sMLONZtYL/AA4p6jMOcBtIf1j4C2qxarwgJk9a2arQ/pl4ElgZi3OXUXOAb5rESuBAyTNqIMdbwGeMbPRPhBaFczsQeDFouz4PXYb8M4SVd8G3GdmL5rZduA+YGkt7DOzFWaWC5srgcOqfd6RUqb9RsJIvutjZm/2hd+N9wD/Ve3z1goXjoiZwJ9j25sY/sM8UCZ8eV4CDqqJdTGCi2wh8EiJ3SdI6pT0S0nzamoYGLBC0mOSLimxfyRtXAvOp/wXtp7tB3ComT0L0Z8F4JASZcZLO15M1IMsxb7uhSS5LLjSbi7j6hsP7Xcy8LyZPV1mfz3bb0S4cESU6jkUTzcbSZlEkTQF+AlwuZntLNq9msj9Mh/4BrC8lrYBJ5nZIuAM4O8k/U3R/vHQfk3A2cCPSuyud/uNlPHQjp8DcsCyMkX2dS8kxb8DrwEWAM8SuYOKqXv7ARew995GvdpvxLhwRGwCDo9tHwZsLldGUgaYyui6yqNCUpZINJaZ2R3F+81sp5ntCum7gaykg2tln5ltDu9bgDuJXAJxRtLGSXMGsNrMni/eUe/2CzxfcN+F9y0lytS1HcNg/FnA+yw45IsZwb2QCGb2vJnlzawf+HaZ89a7/TLA3wK3lytTr/arBBeOiEeBOZJmh3+l5wN3FZW5CyjMYDkX+FW5L061CT7R7wBPmtlXypR5ZWHMRdLxRJ/tthrZN1lSWyFNNIj6RFGxu4APhtlVbwReKrhlakjZf3r1bL8Y8XvsQuCnJcrcCyyRdGBwxSwJeYkjaSnwGeBsM+sqU2Yk90JS9sXHzN5V5rwj+a4nyenAU2a2qdTOerZfRdR7dH68vIhm/fyBaMbF50Le1URfEoBJRC6ODcDvgSNraNubiLrTa4CO8DoTuBS4NJS5DFhLNEtkJXBiDe07Mpy3M9hQaL+4fQJuCO37OLC4xp9vK5EQTI3l1a39iATsWaCP6F/wh4nGzO4Hng7v00LZxcBNsboXh/twA/ChGtq3gWh8oHAPFmYZvgq4e2/3Qo3s+164t9YQicGMYvvC9rDvei3sC/m3Fu65WNmat99YX/7kuOM4jlMR7qpyHMdxKsKFw3Ecx6kIFw7HcRynIlw4HMdxnIpw4XAcx3EqwoXDcRzHqQgXDmdcIunh8H6EpPfW4HxnJxViewTn/mqSYSUkXS3p9FHWXVAuPPkI6k6XdM9o6jrjG3+OwxnXSDqFKFT2WRXUSZtZPjmrqoekaUQPf72x3raUQtJFRA9rXjbK+rcQPbz4UFUNc+qK9ziccYmkXSF5DXByWNTmHySlw4JCj4YoqB8N5U9RtNjV94meHkbS8hBhdG08ymhYyGd1iIR7f8i7SNI3Q/rVku4Px79f0qyQf6uixagelrRR0rmxY14Rs+lfQt5kSb8I53lC0nklLvVc4J7YcV4v6X+C3ffGYlc9IOlaSb+X9AdJJ5dpt39UtAhQp6RrYnafW+nxQ0iOq4HzQvufp2ixqeXhOldKOi7Uf7MGFylqL4TNIAoW+b4RfuzORKHej677y1+lXsCu8H4KsQWWgEuAz4d0M7AKmB3K7QZmx8oWQna0EMX7OQiYThQ2Y3ZRmYuAb4b0z4ALQ/piYHlI30oUdiYFHEO0rgNE8YRuJAqrkgJ+TrSQz7uBb8fsmVriOm8D3hHSWeBhYHrYPg+4OaQfAL4c0mcC/13iWGeE+q1F13YrkUBVfPx4u4TtbwD/HNKnAR2xNjsppKcwuODTTODxet9P/qruK4PjTCyWAMfF/u1PBeYAvcDvzeyPsbKflPSukD48lJsOPFgoZ2alIhyfQBTBFKL4R9fF9i23KPrqOkmHxmxaArSH7SnhXL8BviTpWiLx+02Jc80Atob0UcCxwH0h3mKaKN5RgUJU5MeAI0oc63TgFgsBCEtc21iPD1HctHeH4/9K0kGSpgIPAV+RtAy4wwaD+G0hisXkNBAuHM5EQ8AnzGxIRNgwFrK7aPt04AQz65L0AFGgSlH5+gvx8j1FthTev2hm3xpmrPR6on/wX5S0wsyuLirSHewqHGetmZ1Qxo7CufOU/u7u69rGevzCMYoxM7tG0i+IrnWlpNPN7Cmia+vei03OBMTHOJzxzstE66wXuBf4mKL1SZA0N4SfLmYqsD2IxmuJ1jkH+B3wZkmzQ/1pJeo+TBRuGyL//G/3YeO9wMWKFtpC0kxJh0h6FdBlZv8JfIloDepingT+KqTXA9MlnRCOk1VlKxGuCHa0hvrF1zaa4xe3/4OEMYsgzi+Y2U5JrzGzx83sWiL34WtD+bmMx7DgzpjwHocz3lkD5CR1Evnqv0bkRlmtyN+yldJrc98DXCppDdEP5koAM9saBsrvkJQicqW8tajuJ4GbJV0Rjv+hvRloZiskHQ38LriAdgHvJxKE6yX1E4XX/liJ6r8APko086g3uOC+Htw/GeCrROG194mZ3SNpAbBKUi9wN/BPsf2jOf6vgSsldQBfBK4Cbgnt2sXg+iGXSzqVqLeyjsFlZU8N1+g0ED4d13HqjKTfAmeZ2Y5621JtJD0InGNm2+tti1M9XDgcp85IegPQbWZr6m1LNZE0nWim1Xhdv90ZJS4cjuM4TkX44LjjOI5TES4cjuM4TkW4cDiO4zgV4cLhOI7jVIQLh+M4jlMR/w9ki142xqMFiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasas = [0.01, 1e-3, 9e-5, 1e-6, 1e-7, 1e-10, 2e-20]\n",
    "modelos = {}\n",
    "for i in tasas:\n",
    "    print (\"La tasa de aprendizaje es: \" + str(i))\n",
    "    modelos[str(i)] = modelo(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 2000, tasa = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in tasas:\n",
    "    plt.plot(np.squeeze(modelos[str(i)][\"Costes\"]), label= str(modelos[str(i)][\"Tasa de aprendizaje\"]))\n",
    "\n",
    "plt.ylabel('coste')\n",
    "plt.xlabel('iteraciones (en cientos)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2.8\n",
    "\n",
    "Analice los resultados, con cuál tasa de aprendizaje intentaría mejorar el desempeño del modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentaría mejorar el modelo con la tasa de aprendizaje establecida en 1e-7, dado que representa el menor coste a menor numero de iteraciones, y se obtiene el accuracy máximo.\n"
     ]
    }
   ],
   "source": [
    "print('Intentaría mejorar el modelo con la tasa de aprendizaje establecida en 1e-7, dado que representa el menor coste a menor numero de iteraciones, y se obtiene el accuracy máximo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparacion con la implementación tradicional de regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ajustamos el modelo logístico y lo probamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergio\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logT = LogisticRegression(penalty='l1', max_iter=1500)\n",
    "logT.fit(CE_x, CE_y)\n",
    "y_tr = logT.predict(CE_x)\n",
    "y_pred = logT.predict(CP_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinemos los coeficientes del modelo de la neurona sigmoide y su desviación con respecto a la estimación tradicional de regresion logistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=61</i>\n",
       "<table id=\"table2839090175840\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>Regresion logistica [1]</th><th>Neurona sigmoide [1]</th><th>Diferencia [1]</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>0.02235241504866487</td><td>-2.478349949724639e-05</td><td>0.022377198548162117</td></tr>\n",
       "<tr><td>7.477152340557807e-05</td><td>-0.0001450843860869214</td><td>0.00021985590949249946</td></tr>\n",
       "<tr><td>0.267510676815867</td><td>-1.294443442390108e-05</td><td>0.2675236212502909</td></tr>\n",
       "<tr><td>0.0</td><td>-1.3019209233839786e-05</td><td>1.3019209233839786e-05</td></tr>\n",
       "<tr><td>-0.00906612175833068</td><td>-0.00017710568746503875</td><td>-0.008889016070865641</td></tr>\n",
       "<tr><td>0.1591265553495367</td><td>-6.464349881986249e-06</td><td>0.1591330196994187</td></tr>\n",
       "<tr><td>0.1449694180669084</td><td>-5.135040274195558e-06</td><td>0.1449745531071826</td></tr>\n",
       "<tr><td>0.28656654900597894</td><td>9.740540904311402e-07</td><td>0.2865655749518885</td></tr>\n",
       "<tr><td>0.0</td><td>1.1100859462191477e-07</td><td>-1.1100859462191477e-07</td></tr>\n",
       "<tr><td>-0.35816864708767493</td><td>-4.209644178383274e-07</td><td>-0.3581682261232571</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>-0.26980053616912625</td><td>-5.138272406479793e-06</td><td>-0.26979539789671975</td></tr>\n",
       "<tr><td>-0.48949125069173105</td><td>1.5533114786927503e-07</td><td>-0.48949140602287894</td></tr>\n",
       "<tr><td>0.0</td><td>6.389796595959118e-08</td><td>-6.389796595959118e-08</td></tr>\n",
       "<tr><td>0.0</td><td>-1.1281662284803572e-06</td><td>1.1281662284803572e-06</td></tr>\n",
       "<tr><td>0.0</td><td>-3.4779869255604774e-06</td><td>3.4779869255604774e-06</td></tr>\n",
       "<tr><td>-0.07410451745291904</td><td>-7.122565691860779e-08</td><td>-0.07410444622726213</td></tr>\n",
       "<tr><td>0.0</td><td>-2.5639715125493715e-06</td><td>2.5639715125493715e-06</td></tr>\n",
       "<tr><td>-0.30577672213752394</td><td>-2.0495093324504682e-06</td><td>-0.30577467262819147</td></tr>\n",
       "<tr><td>0.0</td><td>-3.8893067994448015e-06</td><td>3.8893067994448015e-06</td></tr>\n",
       "<tr><td>-1.3773674914644933</td><td>-7.241740455550514e-07</td><td>-1.3773667672904477</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=61>\n",
       "Regresion logistica [1]   Neurona sigmoide [1]       Diferencia [1]    \n",
       "        float64                 float64                 float64        \n",
       "----------------------- ----------------------- -----------------------\n",
       "    0.02235241504866487  -2.478349949724639e-05    0.022377198548162117\n",
       "  7.477152340557807e-05  -0.0001450843860869214  0.00021985590949249946\n",
       "      0.267510676815867  -1.294443442390108e-05      0.2675236212502909\n",
       "                    0.0 -1.3019209233839786e-05  1.3019209233839786e-05\n",
       "   -0.00906612175833068 -0.00017710568746503875   -0.008889016070865641\n",
       "     0.1591265553495367  -6.464349881986249e-06      0.1591330196994187\n",
       "     0.1449694180669084  -5.135040274195558e-06      0.1449745531071826\n",
       "    0.28656654900597894   9.740540904311402e-07      0.2865655749518885\n",
       "                    0.0  1.1100859462191477e-07 -1.1100859462191477e-07\n",
       "   -0.35816864708767493  -4.209644178383274e-07     -0.3581682261232571\n",
       "                    ...                     ...                     ...\n",
       "   -0.26980053616912625  -5.138272406479793e-06    -0.26979539789671975\n",
       "   -0.48949125069173105  1.5533114786927503e-07    -0.48949140602287894\n",
       "                    0.0   6.389796595959118e-08  -6.389796595959118e-08\n",
       "                    0.0 -1.1281662284803572e-06  1.1281662284803572e-06\n",
       "                    0.0 -3.4779869255604774e-06  3.4779869255604774e-06\n",
       "   -0.07410451745291904  -7.122565691860779e-08    -0.07410444622726213\n",
       "                    0.0 -2.5639715125493715e-06  2.5639715125493715e-06\n",
       "   -0.30577672213752394 -2.0495093324504682e-06    -0.30577467262819147\n",
       "                    0.0 -3.8893067994448015e-06  3.8893067994448015e-06\n",
       "    -1.3773674914644933  -7.241740455550514e-07     -1.3773667672904477"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from astropy.table import QTable, Table, Column\n",
    "\n",
    "Tabla =  Table([logT.coef_.T, d['w'], logT.coef_.T-d['w']], names=(\"Regresion logistica\", \"Neurona sigmoide\", \"Diferencia\"))\n",
    "Tabla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3.1\n",
    "\n",
    "Qué puede observar en esta comparativa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La tabla muestra una diferencia respecto a las magnitudes de los coeficientes, en la neurona sigmoide los coeficientes están relacionados con valores relativos a 1e-5\n"
     ]
    }
   ],
   "source": [
    "print('La tabla muestra una diferencia respecto a las magnitudes de los coeficientes, en la neurona sigmoide los coeficientes están relacionados con valores relativos a 1e-5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la exactitud de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La neurona sigmoide tiene una exactitud de entrenamiento: 0.7 y de validacion: 0.7\n",
      "La regresion tradicional tiene una exactitud de entrenamiento: 0.765 y de validacion: 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"La neurona sigmoide tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((d['Prediccion_entrenamiento'] == CE_y2).mean())) +\" y de validacion: \" +str(float((d['Prediccion_prueba'] == CP_y2).mean())))\n",
    "print(\"La regresion tradicional tiene una exactitud de entrenamiento: \" \n",
    "      +str(float((y_tr == CE_y).mean())) +\" y de validacion: \" +str(float((y_pred == CP_y).mean())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio  3.2\n",
    "\n",
    "Ahora puede desarrollar su propio código intentando mejorar los resultados obtenidos. \n",
    "\n",
    "Intente sobrepasar los resultados de la regresion logistica tradicional. Optimice la tasa de aprendizaje, el número de iteraciones o (bono) investigue y cambie la manera en la cual inicializamos los coeficientes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo inicializando con aleatorios divididos por 100\n",
    "def modelo2(CE_x, CP_x, CE_y, CP_y, num_iter, tasa, print_cost):\n",
    "    \"\"\"\n",
    "    Construye el modelo de regresión logística llamando las funciones implementadas anteriormente\n",
    "    Output:\n",
    "    d: diccionario con la información sobre el modelo.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicialice los parametros con ceros \n",
    "    w = np.zeros([CE_x.shape[0],1])\n",
    "    for i in range (61):\n",
    "        w[i] = np.random.rand(1)/100\n",
    "\n",
    "    b = 0\n",
    "\n",
    "    # Descenso en la dirección del gradiente (GD) \n",
    "    params, grads, costes = optimiza(w, b, CE_x, CE_y, num_iter, tasa, print_cost)\n",
    "    \n",
    "    # Recupere los parámetros w y b del diccionario \"params\" ##\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Prediga los ejemplos de prueba y entrenamiento (≈ 2 líneas de código)\n",
    "    YP_pred = pred(w, b, CP_x)\n",
    "    YE_pred = pred(w, b, CE_x)\n",
    "\n",
    "    # Imprima los errores de entrenamiento y prueba\n",
    "    print(\"Accuracy de entrenamiento: {} %\".format(100 - np.mean(np.abs(YE_pred - CE_y)) * 100))\n",
    "    print(\"Accuracy de prueba: {} %\".format(100 - np.mean(np.abs(YP_pred - CP_y)) * 100))\n",
    "\n",
    "    \n",
    "    d2 = {\"Costes\": costes,\n",
    "         \"Prediccion_prueba\": YP_pred, \n",
    "         \"Prediccion_entrenamiento\" : YE_pred, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"Tasa de aprendizaje\" : tasa,\n",
    "         \"Numero de iteraciones\": num_iter}\n",
    "    \n",
    "    return d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coste tras la iteración 0: 2.647342\n",
      "Coste tras la iteración 100: 0.696190\n",
      "Coste tras la iteración 200: 0.695948\n",
      "Coste tras la iteración 300: 0.695707\n",
      "Coste tras la iteración 400: 0.695467\n",
      "Coste tras la iteración 500: 0.695228\n",
      "Coste tras la iteración 600: 0.694989\n",
      "Coste tras la iteración 700: 0.694751\n",
      "Coste tras la iteración 800: 0.694513\n",
      "Coste tras la iteración 900: 0.694276\n",
      "Coste tras la iteración 1000: 0.694040\n",
      "Coste tras la iteración 1100: 0.693804\n",
      "Coste tras la iteración 1200: 0.693569\n",
      "Coste tras la iteración 1300: 0.693335\n",
      "Coste tras la iteración 1400: 0.693101\n",
      "Coste tras la iteración 1500: 0.692868\n",
      "Coste tras la iteración 1600: 0.692636\n",
      "Coste tras la iteración 1700: 0.692404\n",
      "Coste tras la iteración 1800: 0.692173\n",
      "Coste tras la iteración 1900: 0.691943\n",
      "Coste tras la iteración 2000: 0.691713\n",
      "Coste tras la iteración 2100: 0.691484\n",
      "Coste tras la iteración 2200: 0.691256\n",
      "Coste tras la iteración 2300: 0.691028\n",
      "Coste tras la iteración 2400: 0.690801\n",
      "Coste tras la iteración 2500: 0.690574\n",
      "Coste tras la iteración 2600: 0.690348\n",
      "Coste tras la iteración 2700: 0.690123\n",
      "Coste tras la iteración 2800: 0.689898\n",
      "Coste tras la iteración 2900: 0.689674\n",
      "Coste tras la iteración 3000: 0.689450\n",
      "Coste tras la iteración 3100: 0.689228\n",
      "Coste tras la iteración 3200: 0.689005\n",
      "Coste tras la iteración 3300: 0.688784\n",
      "Coste tras la iteración 3400: 0.688563\n",
      "Coste tras la iteración 3500: 0.688343\n",
      "Coste tras la iteración 3600: 0.688123\n",
      "Coste tras la iteración 3700: 0.687904\n",
      "Coste tras la iteración 3800: 0.687685\n",
      "Coste tras la iteración 3900: 0.687467\n",
      "Coste tras la iteración 4000: 0.687250\n",
      "Coste tras la iteración 4100: 0.687033\n",
      "Coste tras la iteración 4200: 0.686817\n",
      "Coste tras la iteración 4300: 0.686602\n",
      "Coste tras la iteración 4400: 0.686387\n",
      "Coste tras la iteración 4500: 0.686173\n",
      "Coste tras la iteración 4600: 0.685959\n",
      "Coste tras la iteración 4700: 0.685746\n",
      "Coste tras la iteración 4800: 0.685534\n",
      "Coste tras la iteración 4900: 0.685322\n",
      "Coste tras la iteración 5000: 0.685110\n",
      "Coste tras la iteración 5100: 0.684900\n",
      "Coste tras la iteración 5200: 0.684690\n",
      "Coste tras la iteración 5300: 0.684480\n",
      "Coste tras la iteración 5400: 0.684271\n",
      "Coste tras la iteración 5500: 0.684063\n",
      "Coste tras la iteración 5600: 0.683855\n",
      "Coste tras la iteración 5700: 0.683648\n",
      "Coste tras la iteración 5800: 0.683442\n",
      "Coste tras la iteración 5900: 0.683236\n",
      "Coste tras la iteración 6000: 0.683030\n",
      "Coste tras la iteración 6100: 0.682825\n",
      "Coste tras la iteración 6200: 0.682621\n",
      "Coste tras la iteración 6300: 0.682417\n",
      "Coste tras la iteración 6400: 0.682214\n",
      "Coste tras la iteración 6500: 0.682012\n",
      "Coste tras la iteración 6600: 0.681810\n",
      "Coste tras la iteración 6700: 0.681609\n",
      "Coste tras la iteración 6800: 0.681408\n",
      "Coste tras la iteración 6900: 0.681208\n",
      "Coste tras la iteración 7000: 0.681008\n",
      "Coste tras la iteración 7100: 0.680809\n",
      "Coste tras la iteración 7200: 0.680610\n",
      "Coste tras la iteración 7300: 0.680412\n",
      "Coste tras la iteración 7400: 0.680215\n",
      "Coste tras la iteración 7500: 0.680018\n",
      "Coste tras la iteración 7600: 0.679822\n",
      "Coste tras la iteración 7700: 0.679626\n",
      "Coste tras la iteración 7800: 0.679431\n",
      "Coste tras la iteración 7900: 0.679236\n",
      "Coste tras la iteración 8000: 0.679042\n",
      "Coste tras la iteración 8100: 0.678848\n",
      "Coste tras la iteración 8200: 0.678655\n",
      "Coste tras la iteración 8300: 0.678463\n",
      "Coste tras la iteración 8400: 0.678271\n",
      "Coste tras la iteración 8500: 0.678080\n",
      "Coste tras la iteración 8600: 0.677889\n",
      "Coste tras la iteración 8700: 0.677699\n",
      "Coste tras la iteración 8800: 0.677509\n",
      "Coste tras la iteración 8900: 0.677320\n",
      "Coste tras la iteración 9000: 0.677131\n",
      "Coste tras la iteración 9100: 0.676943\n",
      "Coste tras la iteración 9200: 0.676755\n",
      "Coste tras la iteración 9300: 0.676568\n",
      "Coste tras la iteración 9400: 0.676382\n",
      "Coste tras la iteración 9500: 0.676196\n",
      "Coste tras la iteración 9600: 0.676010\n",
      "Coste tras la iteración 9700: 0.675825\n",
      "Coste tras la iteración 9800: 0.675641\n",
      "Coste tras la iteración 9900: 0.675457\n",
      "Coste tras la iteración 10000: 0.675274\n",
      "Coste tras la iteración 10100: 0.675091\n",
      "Coste tras la iteración 10200: 0.674909\n",
      "Coste tras la iteración 10300: 0.674727\n",
      "Coste tras la iteración 10400: 0.674545\n",
      "Coste tras la iteración 10500: 0.674365\n",
      "Coste tras la iteración 10600: 0.674184\n",
      "Coste tras la iteración 10700: 0.674005\n",
      "Coste tras la iteración 10800: 0.673825\n",
      "Coste tras la iteración 10900: 0.673647\n",
      "Coste tras la iteración 11000: 0.673469\n",
      "Coste tras la iteración 11100: 0.673291\n",
      "Coste tras la iteración 11200: 0.673114\n",
      "Coste tras la iteración 11300: 0.672937\n",
      "Coste tras la iteración 11400: 0.672761\n",
      "Coste tras la iteración 11500: 0.672585\n",
      "Coste tras la iteración 11600: 0.672410\n",
      "Coste tras la iteración 11700: 0.672235\n",
      "Coste tras la iteración 11800: 0.672061\n",
      "Coste tras la iteración 11900: 0.671888\n",
      "Coste tras la iteración 12000: 0.671714\n",
      "Coste tras la iteración 12100: 0.671542\n",
      "Coste tras la iteración 12200: 0.671370\n",
      "Coste tras la iteración 12300: 0.671198\n",
      "Coste tras la iteración 12400: 0.671027\n",
      "Coste tras la iteración 12500: 0.670856\n",
      "Coste tras la iteración 12600: 0.670686\n",
      "Coste tras la iteración 12700: 0.670516\n",
      "Coste tras la iteración 12800: 0.670347\n",
      "Coste tras la iteración 12900: 0.670178\n",
      "Coste tras la iteración 13000: 0.670010\n",
      "Coste tras la iteración 13100: 0.669842\n",
      "Coste tras la iteración 13200: 0.669675\n",
      "Coste tras la iteración 13300: 0.669508\n",
      "Coste tras la iteración 13400: 0.669341\n",
      "Coste tras la iteración 13500: 0.669176\n",
      "Coste tras la iteración 13600: 0.669010\n",
      "Coste tras la iteración 13700: 0.668845\n",
      "Coste tras la iteración 13800: 0.668681\n",
      "Coste tras la iteración 13900: 0.668517\n",
      "Coste tras la iteración 14000: 0.668353\n",
      "Coste tras la iteración 14100: 0.668190\n",
      "Coste tras la iteración 14200: 0.668028\n",
      "Coste tras la iteración 14300: 0.667866\n",
      "Coste tras la iteración 14400: 0.667704\n",
      "Coste tras la iteración 14500: 0.667543\n",
      "Coste tras la iteración 14600: 0.667382\n",
      "Coste tras la iteración 14700: 0.667222\n",
      "Coste tras la iteración 14800: 0.667062\n",
      "Coste tras la iteración 14900: 0.666903\n",
      "Coste tras la iteración 15000: 0.666744\n",
      "Coste tras la iteración 15100: 0.666585\n",
      "Coste tras la iteración 15200: 0.666427\n",
      "Coste tras la iteración 15300: 0.666270\n",
      "Coste tras la iteración 15400: 0.666113\n",
      "Coste tras la iteración 15500: 0.665956\n",
      "Coste tras la iteración 15600: 0.665800\n",
      "Coste tras la iteración 15700: 0.665644\n",
      "Coste tras la iteración 15800: 0.665489\n",
      "Coste tras la iteración 15900: 0.665334\n",
      "Coste tras la iteración 16000: 0.665180\n",
      "Coste tras la iteración 16100: 0.665026\n",
      "Coste tras la iteración 16200: 0.664873\n",
      "Coste tras la iteración 16300: 0.664720\n",
      "Coste tras la iteración 16400: 0.664567\n",
      "Coste tras la iteración 16500: 0.664415\n",
      "Coste tras la iteración 16600: 0.664263\n",
      "Coste tras la iteración 16700: 0.664112\n",
      "Coste tras la iteración 16800: 0.663961\n",
      "Coste tras la iteración 16900: 0.663811\n",
      "Coste tras la iteración 17000: 0.663661\n",
      "Coste tras la iteración 17100: 0.663511\n",
      "Coste tras la iteración 17200: 0.663362\n",
      "Coste tras la iteración 17300: 0.663213\n",
      "Coste tras la iteración 17400: 0.663065\n",
      "Coste tras la iteración 17500: 0.662917\n",
      "Coste tras la iteración 17600: 0.662770\n",
      "Coste tras la iteración 17700: 0.662623\n",
      "Coste tras la iteración 17800: 0.662476\n",
      "Coste tras la iteración 17900: 0.662330\n",
      "Coste tras la iteración 18000: 0.662185\n",
      "Coste tras la iteración 18100: 0.662039\n",
      "Coste tras la iteración 18200: 0.661895\n",
      "Coste tras la iteración 18300: 0.661750\n",
      "Coste tras la iteración 18400: 0.661606\n",
      "Coste tras la iteración 18500: 0.661462\n",
      "Coste tras la iteración 18600: 0.661319\n",
      "Coste tras la iteración 18700: 0.661176\n",
      "Coste tras la iteración 18800: 0.661034\n",
      "Coste tras la iteración 18900: 0.660892\n",
      "Coste tras la iteración 19000: 0.660751\n",
      "Coste tras la iteración 19100: 0.660609\n",
      "Coste tras la iteración 19200: 0.660469\n",
      "Coste tras la iteración 19300: 0.660328\n",
      "Coste tras la iteración 19400: 0.660189\n",
      "Coste tras la iteración 19500: 0.660049\n",
      "Coste tras la iteración 19600: 0.659910\n",
      "Coste tras la iteración 19700: 0.659771\n",
      "Coste tras la iteración 19800: 0.659633\n",
      "Coste tras la iteración 19900: 0.659495\n",
      "Accuracy de entrenamiento: 70.0 %\n",
      "Accuracy de prueba: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "mejora = modelo2(CE_x2, CP_x2, CE_y2, CP_y2, num_iter = 20000, tasa = 1e-7, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejora de modelo\n",
    "\n",
    "\n",
    "Se intentó mejorar del modelo a través de la inicialización aleatoria de las ponderaciones (valores w), en este caso, justificado bajo el supuesto que el modelo al iniciar en valores 0, puede buscar óptimos locales, ignorando ciertas restricciones y combinaciones lineales que lo puedan llevar a un óptimo global.\n",
    "\n",
    "Al inicializar estos valores en un aleatorio, se le fija un punto de partida distinto en el hiperplano para poder realizar una búsqueda de un valor óptimo (Sea local o global) que permita encontrar un accuracy más alto)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
